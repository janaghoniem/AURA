{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ff45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library-Specific RAG Data Collection System\n",
    "# Comprehensive data collection from Documentation, GitHub, and StackOverflow\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf18b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\yusr\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\yusr\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.0 MB 3.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/11.0 MB 3.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 1.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 1.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.6/11.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.4/11.0 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/11.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.8/11.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.1/11.0 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.3/11.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 2.7 MB/s  0:00:04\n",
      "Downloading numpy-2.3.5-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.9/12.8 MB 4.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.5/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.1/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.9/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.7/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 3.6 MB/s  0:00:03\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ---------------------------------------- 4/4 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.5 pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install requests\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install bs4\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bcf9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 1: Base Configuration and Utilities\n",
    "# ============================================================================\n",
    "\n",
    "class DataCollectionConfig:\n",
    "    \"\"\"Configuration for data collection\"\"\"\n",
    "    \n",
    "    def __init__(self, library_name: str):\n",
    "        self.library_name = library_name\n",
    "        self.base_dir = Path(f\"rag_data/{library_name}\")\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Output directories\n",
    "        self.docs_dir = self.base_dir / \"documentation\"\n",
    "        self.github_dir = self.base_dir / \"github\"\n",
    "        self.stackoverflow_dir = self.base_dir / \"stackoverflow\"\n",
    "        self.combined_dir = self.base_dir / \"combined\"\n",
    "        \n",
    "        for d in [self.docs_dir, self.github_dir, self.stackoverflow_dir, self.combined_dir]:\n",
    "            d.mkdir(exist_ok=True)\n",
    "    \n",
    "    def get_output_path(self, source: str, filename: str) -> Path:\n",
    "        \"\"\"Get output path for a specific source\"\"\"\n",
    "        source_map = {\n",
    "            'docs': self.docs_dir,\n",
    "            'github': self.github_dir,\n",
    "            'stackoverflow': self.stackoverflow_dir,\n",
    "            'combined': self.combined_dir\n",
    "        }\n",
    "        return source_map[source] / filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53438fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 2: Documentation Scraper\n",
    "# ============================================================================\n",
    "\n",
    "class DocumentationScraper:\n",
    "    \"\"\"Scrape library documentation from ReadTheDocs or similar sites\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataCollectionConfig):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.visited_urls = set()\n",
    "        self.collected_data = []\n",
    "    \n",
    "    def scrape_page(self, url: str, max_depth: int = 3, current_depth: int = 0) -> List[Dict]:\n",
    "        \"\"\"Recursively scrape documentation pages\"\"\"\n",
    "        if current_depth > max_depth or url in self.visited_urls:\n",
    "            return []\n",
    "        \n",
    "        self.visited_urls.add(url)\n",
    "        print(f\"Scraping: {url} (depth: {current_depth})\")\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract main content\n",
    "            content_section = soup.find('div', {'role': 'main'}) or soup.find('article') or soup.find('main')\n",
    "            \n",
    "            if content_section:\n",
    "                # Extract text content\n",
    "                text_content = content_section.get_text(separator='\\n', strip=True)\n",
    "                \n",
    "                # Extract code blocks\n",
    "                code_blocks = []\n",
    "                for code_tag in content_section.find_all(['code', 'pre']):\n",
    "                    code_text = code_tag.get_text(strip=True)\n",
    "                    if len(code_text) > 20:  # Filter out inline code\n",
    "                        code_blocks.append(code_text)\n",
    "                \n",
    "                # Extract headers for structure\n",
    "                headers = [h.get_text(strip=True) for h in content_section.find_all(['h1', 'h2', 'h3', 'h4'])]\n",
    "                \n",
    "                page_data = {\n",
    "                    'id': f\"doc_{len(self.collected_data)}\",\n",
    "                    'type': 'documentation',\n",
    "                    'library': self.config.library_name,\n",
    "                    'source': 'documentation',\n",
    "                    'source_url': url,\n",
    "                    'title': soup.find('title').get_text(strip=True) if soup.find('title') else '',\n",
    "                    'headers': headers,\n",
    "                    'content': text_content[:5000],  # Limit content length\n",
    "                    'code_blocks': code_blocks,\n",
    "                    'collected_at': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                self.collected_data.append(page_data)\n",
    "                \n",
    "                # Find and follow internal links\n",
    "                if current_depth < max_depth:\n",
    "                    for link in content_section.find_all('a', href=True):\n",
    "                        next_url = urljoin(url, link['href'])\n",
    "                        # Only follow links within the same domain\n",
    "                        if urlparse(next_url).netloc == urlparse(url).netloc:\n",
    "                            if next_url not in self.visited_urls:\n",
    "                                time.sleep(0.5)  # Rate limiting\n",
    "                                self.scrape_page(next_url, max_depth, current_depth + 1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "        \n",
    "        return self.collected_data\n",
    "    \n",
    "    def save_data(self):\n",
    "        \"\"\"Save collected documentation data\"\"\"\n",
    "        output_file = self.config.get_output_path('docs', f'{self.config.library_name}_docs.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.collected_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved {len(self.collected_data)} documentation pages to {output_file}\")\n",
    "        return output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6226cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 3: GitHub Repository Scraper\n",
    "# ============================================================================\n",
    "\n",
    "class GitHubScraper:\n",
    "    \"\"\"Scrape code examples and documentation from GitHub repositories\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataCollectionConfig, github_token: str = None):\n",
    "        self.config = config\n",
    "        self.github_token = github_token\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        headers = {'User-Agent': 'RAG-Data-Collector'}\n",
    "        if github_token:\n",
    "            headers['Authorization'] = f'token {github_token}'\n",
    "        self.session.headers.update(headers)\n",
    "        \n",
    "        self.collected_data = []\n",
    "    \n",
    "    def get_repo_contents(self, owner: str, repo: str, path: str = '') -> List[Dict]:\n",
    "        \"\"\"Get repository contents recursively\"\"\"\n",
    "        url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            contents = response.json()\n",
    "            \n",
    "            if not isinstance(contents, list):\n",
    "                contents = [contents]\n",
    "            \n",
    "            return contents\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching repo contents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_repository(self, repo_url: str, include_patterns: List[str] = None):\n",
    "        \"\"\"Scrape code examples from a GitHub repository\"\"\"\n",
    "        # Parse repo URL\n",
    "        parts = repo_url.rstrip('/').split('/')\n",
    "        owner, repo = parts[-2], parts[-1]\n",
    "        \n",
    "        print(f\"Scraping GitHub repo: {owner}/{repo}\")\n",
    "        \n",
    "        if include_patterns is None:\n",
    "            include_patterns = ['*.py', '*.md', 'examples/*', 'docs/*']\n",
    "        \n",
    "        def process_contents(contents, current_path=''):\n",
    "            for item in contents:\n",
    "                if item['type'] == 'file':\n",
    "                    # Check if file matches include patterns\n",
    "                    if any(self._match_pattern(item['name'], pattern) for pattern in include_patterns):\n",
    "                        self._process_file(owner, repo, item, current_path)\n",
    "                elif item['type'] == 'dir':\n",
    "                    # Recursively process directories\n",
    "                    subcontents = self.get_repo_contents(owner, repo, item['path'])\n",
    "                    time.sleep(0.3)  # Rate limiting\n",
    "                    process_contents(subcontents, item['path'])\n",
    "        \n",
    "        # Start processing from root\n",
    "        root_contents = self.get_repo_contents(owner, repo)\n",
    "        process_contents(root_contents)\n",
    "    \n",
    "    def _match_pattern(self, filename: str, pattern: str) -> bool:\n",
    "        \"\"\"Match filename against pattern\"\"\"\n",
    "        if '*' in pattern:\n",
    "            import fnmatch\n",
    "            return fnmatch.fnmatch(filename, pattern.split('/')[-1])\n",
    "        return pattern in filename\n",
    "    \n",
    "    def _process_file(self, owner: str, repo: str, file_item: Dict, current_path: str):\n",
    "        \"\"\"Process individual file from repository\"\"\"\n",
    "        try:\n",
    "            # Download file content\n",
    "            response = self.session.get(file_item['download_url'])\n",
    "            response.raise_for_status()\n",
    "            content = response.text\n",
    "            \n",
    "            file_data = {\n",
    "                'id': f\"github_{len(self.collected_data)}\",\n",
    "                'type': 'github_code',\n",
    "                'library': self.config.library_name,\n",
    "                'source': 'github',\n",
    "                'source_url': file_item['html_url'],\n",
    "                'repo': f\"{owner}/{repo}\",\n",
    "                'file_path': file_item['path'],\n",
    "                'file_name': file_item['name'],\n",
    "                'content': content[:10000],  # Limit size\n",
    "                'language': self._detect_language(file_item['name']),\n",
    "                'collected_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            self.collected_data.append(file_data)\n",
    "            print(f\"  Collected: {file_item['path']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {file_item['path']}: {e}\")\n",
    "    \n",
    "    def _detect_language(self, filename: str) -> str:\n",
    "        \"\"\"Detect programming language from filename\"\"\"\n",
    "        ext_map = {\n",
    "            '.py': 'python',\n",
    "            '.js': 'javascript',\n",
    "            '.java': 'java',\n",
    "            '.cpp': 'cpp',\n",
    "            '.c': 'c',\n",
    "            '.md': 'markdown',\n",
    "            '.rst': 'restructuredtext'\n",
    "        }\n",
    "        ext = Path(filename).suffix.lower()\n",
    "        return ext_map.get(ext, 'unknown')\n",
    "    \n",
    "    def save_data(self):\n",
    "        \"\"\"Save collected GitHub data\"\"\"\n",
    "        output_file = self.config.get_output_path('github', f'{self.config.library_name}_github.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.collected_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved {len(self.collected_data)} GitHub files to {output_file}\")\n",
    "        return output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3fc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 4: StackOverflow Scraper\n",
    "# ============================================================================\n",
    "\n",
    "class StackOverflowScraper:\n",
    "    \"\"\"Scrape code examples and answers from StackOverflow\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataCollectionConfig, api_key: str = None):\n",
    "        self.config = config\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.stackexchange.com/2.3\"\n",
    "        self.collected_data = []\n",
    "    \n",
    "    def search_questions(self, tag: str, max_results: int = 100):\n",
    "        \"\"\"Search for questions with specific tag\"\"\"\n",
    "        print(f\"Searching StackOverflow for tag: {tag}\")\n",
    "        \n",
    "        params = {\n",
    "            'order': 'desc',\n",
    "            'sort': 'votes',\n",
    "            'tagged': tag,\n",
    "            'site': 'stackoverflow',\n",
    "            'pagesize': min(max_results, 100),\n",
    "            'filter': 'withbody'  # Include question body\n",
    "        }\n",
    "        \n",
    "        if self.api_key:\n",
    "            params['key'] = self.api_key\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/questions\", params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            questions = data.get('items', [])\n",
    "            print(f\"Found {len(questions)} questions\")\n",
    "            \n",
    "            for question in questions:\n",
    "                self._process_question(question)\n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching StackOverflow: {e}\")\n",
    "    \n",
    "    def _process_question(self, question: Dict):\n",
    "        \"\"\"Process individual question and its answers\"\"\"\n",
    "        question_id = question['question_id']\n",
    "        \n",
    "        # Get answers for this question\n",
    "        params = {\n",
    "            'order': 'desc',\n",
    "            'sort': 'votes',\n",
    "            'site': 'stackoverflow',\n",
    "            'filter': 'withbody'\n",
    "        }\n",
    "        \n",
    "        if self.api_key:\n",
    "            params['key'] = self.api_key\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{self.base_url}/questions/{question_id}/answers\",\n",
    "                params=params\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            answers = response.json().get('items', [])\n",
    "            \n",
    "            # Extract code from question\n",
    "            question_code = self._extract_code(question.get('body', ''))\n",
    "            \n",
    "            # Extract code from answers\n",
    "            answer_codes = []\n",
    "            for answer in answers:\n",
    "                if answer.get('is_accepted') or answer.get('score', 0) > 0:\n",
    "                    code = self._extract_code(answer.get('body', ''))\n",
    "                    if code:\n",
    "                        answer_codes.extend(code)\n",
    "            \n",
    "            if question_code or answer_codes:\n",
    "                item_data = {\n",
    "                    'id': f\"so_{question_id}\",\n",
    "                    'type': 'stackoverflow',\n",
    "                    'library': self.config.library_name,\n",
    "                    'source': 'stackoverflow',\n",
    "                    'source_url': question['link'],\n",
    "                    'question_id': question_id,\n",
    "                    'title': question['title'],\n",
    "                    'score': question.get('score', 0),\n",
    "                    'tags': question.get('tags', []),\n",
    "                    'question_code': question_code,\n",
    "                    'answer_codes': answer_codes,\n",
    "                    'view_count': question.get('view_count', 0),\n",
    "                    'collected_at': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                self.collected_data.append(item_data)\n",
    "                print(f\"  Collected Q{question_id}: {question['title'][:50]}...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing question {question_id}: {e}\")\n",
    "    \n",
    "    def _extract_code(self, html_content: str) -> List[str]:\n",
    "        \"\"\"Extract code blocks from HTML content\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        code_blocks = []\n",
    "        \n",
    "        for code_tag in soup.find_all('code'):\n",
    "            code_text = code_tag.get_text(strip=True)\n",
    "            if len(code_text) > 20:  # Filter out inline code\n",
    "                code_blocks.append(code_text)\n",
    "        \n",
    "        return code_blocks\n",
    "    \n",
    "    def save_data(self):\n",
    "        \"\"\"Save collected StackOverflow data\"\"\"\n",
    "        output_file = self.config.get_output_path('stackoverflow', f'{self.config.library_name}_stackoverflow.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.collected_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved {len(self.collected_data)} StackOverflow items to {output_file}\")\n",
    "        return output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 5: Data Combiner and Processor\n",
    "# ============================================================================\n",
    "\n",
    "class DataCombiner:\n",
    "    \"\"\"Combine and process data from all sources\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataCollectionConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def combine_all_sources(self) -> pd.DataFrame:\n",
    "        \"\"\"Combine data from all sources into a single dataset\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        # Load documentation data\n",
    "        docs_file = self.config.get_output_path('docs', f'{self.config.library_name}_docs.json')\n",
    "        if docs_file.exists():\n",
    "            with open(docs_file, 'r', encoding='utf-8') as f:\n",
    "                all_data.extend(json.load(f))\n",
    "        \n",
    "        # Load GitHub data\n",
    "        github_file = self.config.get_output_path('github', f'{self.config.library_name}_github.json')\n",
    "        if github_file.exists():\n",
    "            with open(github_file, 'r', encoding='utf-8') as f:\n",
    "                all_data.extend(json.load(f))\n",
    "        \n",
    "        # Load StackOverflow data\n",
    "        so_file = self.config.get_output_path('stackoverflow', f'{self.config.library_name}_stackoverflow.json')\n",
    "        if so_file.exists():\n",
    "            with open(so_file, 'r', encoding='utf-8') as f:\n",
    "                all_data.extend(json.load(f))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Save combined data\n",
    "        combined_json = self.config.get_output_path('combined', f'{self.config.library_name}_combined.json')\n",
    "        with open(combined_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        combined_csv = self.config.get_output_path('combined', f'{self.config.library_name}_combined.csv')\n",
    "        df.to_csv(combined_csv, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\nCombined Data Summary:\")\n",
    "        print(f\"Total items: {len(df)}\")\n",
    "        print(f\"By source: {df['source'].value_counts().to_dict()}\")\n",
    "        print(f\"Saved to: {combined_json}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0164699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: PyWinAuto Specific Collection\n",
    "# ============================================================================\n",
    "\n",
    "def collect_pywinauto_data(github_token: str = None, stackoverflow_key: str = None):\n",
    "    \"\"\"\n",
    "    Collect comprehensive data for pywinauto library\n",
    "    \n",
    "    Args:\n",
    "        github_token: GitHub API token (optional but recommended)\n",
    "        stackoverflow_key: StackOverflow API key (optional)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COLLECTING DATA FOR: pywinauto\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Initialize configuration\n",
    "    config = DataCollectionConfig('pywinauto')\n",
    "    \n",
    "    # # 1. Scrape Documentation\n",
    "    # print(\"\\n[1/4] Scraping Documentation...\")\n",
    "    # doc_scraper = DocumentationScraper(config)\n",
    "    # doc_scraper.scrape_page('https://pywinauto.readthedocs.io/en/latest/contents.html', max_depth=2)\n",
    "    # doc_scraper.save_data()\n",
    "    \n",
    "    # # 2. Scrape GitHub Repository\n",
    "    # print(\"\\n[2/4] Scraping GitHub Repository...\")\n",
    "    # github_scraper = GitHubScraper(config, github_token)\n",
    "    # github_scraper.scrape_repository(\n",
    "    #     'https://github.com/pywinauto/pywinauto',\n",
    "    #     include_patterns=['*.py', '*.md', 'examples/*.py', 'docs/*.rst']\n",
    "    # )\n",
    "    # github_scraper.save_data()\n",
    "    \n",
    "    # 3. Scrape StackOverflow\n",
    "    print(\"\\n[3/4] Scraping StackOverflow...\")\n",
    "    so_scraper = StackOverflowScraper(config, stackoverflow_key)\n",
    "    so_scraper.search_questions('pywinauto', max_results=100)\n",
    "    so_scraper.save_data()\n",
    "    \n",
    "    # 4. Combine all data\n",
    "    print(\"\\n[4/4] Combining all sources...\")\n",
    "    combiner = DataCombiner(config)\n",
    "    combined_df = combiner.combine_all_sources()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA COLLECTION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4fed9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Generic Library Collection Function\n",
    "# ============================================================================\n",
    "\n",
    "def collect_library_data(\n",
    "    library_name: str,\n",
    "    docs_url: str,\n",
    "    github_repo_url: str,\n",
    "    stackoverflow_tag: str,\n",
    "    github_token: str = None,\n",
    "    stackoverflow_key: str = None,\n",
    "    max_so_results: int = 100\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic function to collect data for any library\n",
    "    \n",
    "    Args:\n",
    "        library_name: Name of the library\n",
    "        docs_url: URL to documentation\n",
    "        github_repo_url: GitHub repository URL\n",
    "        stackoverflow_tag: StackOverflow tag to search\n",
    "        github_token: GitHub API token (optional)\n",
    "        stackoverflow_key: StackOverflow API key (optional)\n",
    "        max_so_results: Maximum StackOverflow results to collect\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"COLLECTING DATA FOR: {library_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    config = DataCollectionConfig(library_name)\n",
    "    \n",
    "    # # Documentation\n",
    "    # print(\"\\n[1/4] Scraping Documentation...\")\n",
    "    # doc_scraper = DocumentationScraper(config)\n",
    "    # doc_scraper.scrape_page(docs_url, max_depth=2)\n",
    "    # doc_scraper.save_data()\n",
    "    \n",
    "    # # GitHub\n",
    "    # print(\"\\n[2/4] Scraping GitHub Repository...\")\n",
    "    # github_scraper = GitHubScraper(config, github_token)\n",
    "    # github_scraper.scrape_repository(github_repo_url)\n",
    "    # github_scraper.save_data()\n",
    "    \n",
    "    # StackOverflow\n",
    "    print(\"\\n[3/4] Scraping StackOverflow...\")\n",
    "    so_scraper = StackOverflowScraper(config, stackoverflow_key)\n",
    "    so_scraper.search_questions(stackoverflow_tag, max_results=max_so_results)\n",
    "    so_scraper.save_data()\n",
    "    \n",
    "    # Combine\n",
    "    print(\"\\n[4/4] Combining all sources...\")\n",
    "    combiner = DataCombiner(config)\n",
    "    combined_df = combiner.combine_all_sources()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DATA COLLECTION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077029cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COLLECTING DATA FOR: pywinauto\n",
      "================================================================================\n",
      "\n",
      "[3/4] Scraping StackOverflow...\n",
      "Searching StackOverflow for tag: pywinauto\n",
      "Found 100 questions\n",
      "  Collected Q57523762: pytest - Windows fatal exception: code 0x8001010d...\n",
      "  Collected Q55547940: How to get a list of the name of every open window...\n",
      "  Collected Q65459632: Cannot import pywinauto on Windows 10...\n",
      "  Collected Q39794729: Pywinauto: unable to bring window to foreground...\n",
      "  Collected Q39021888: Using PyWinAuto to control a currently running app...\n",
      "  Collected Q32846550: Python - Control window with pywinauto while the w...\n",
      "  Collected Q24606219: How to perform Click action on Button or Text fiel...\n",
      "  Collected Q40265705: Press key with pywinauto...\n",
      "  Collected Q31367425: pywinauto wait for window to appear and send key-p...\n",
      "  Collected Q58976354: Pywinauto - There are 2 elements that match criter...\n",
      "  Collected Q8008490: How can i find available dialogs ,controls of an a...\n",
      "  Collected Q51284268: WindowsContext: OleInitialize() failed: &quot;COM ...\n",
      "  Collected Q42917783: what is the best way to start google chrome and in...\n",
      "  Collected Q45173450: Interacting with buttons/macros inside excel autom...\n",
      "  Collected Q59505969: Pywinauto timings waiting 0.5 seconds instead of i...\n",
      "  Collected Q48009887: Pywinauto - Can&#39;t connect to office documents ...\n",
      "  Collected Q33019713: How can I send text when the window is minimized?...\n",
      "  Collected Q5039642: How to access the control identifiers in pywinauto...\n",
      "  Collected Q38097476: How to right click on a folder and select from con...\n",
      "  Collected Q45401017: Using pywinauto.top_window() hangs when using it w...\n",
      "  Collected Q15619183: Why can&#39;t I write control identifiers to a fil...\n",
      "  Collected Q38866272: pywinauto.findwindows.WindowNotFoundError in pywin...\n",
      "  Collected Q58333255: Change the active Window...\n",
      "  Collected Q11914152: How to send the TAB button using Python In SendKey...\n",
      "  Collected Q52096146: pywinauto: Iterate through all controls in a windo...\n",
      "  Collected Q34110425: pywinauto 32-bit userwarning...\n",
      "  Collected Q31212228: Checking a checkbox with pywinauto doesn&#39;t wor...\n",
      "  Collected Q25961210: Getting value using pywinauto from text field...\n",
      "  Collected Q65952111: To extract the title name using pywinauto...\n",
      "  Collected Q55928463: ctypes.ArgumentError when using kivy with pywinaut...\n",
      "  Collected Q18982457: Waiting for a application window: pywinauto.timing...\n",
      "  Collected Q21318550: How do I right click on a windows tray icon and cl...\n",
      "  Collected Q29631404: Using PyWinAuto (or anything else) to interact wit...\n",
      "  Collected Q70476128: Pywinauto: How to resize active window...\n",
      "  Collected Q61381863: Python pywinauto Process finished with exit code -...\n",
      "  Collected Q28216222: Python pywinauto search windows with partial title...\n",
      "  Collected Q50268869: Select an item from a combobox using pywinauto...\n",
      "  Collected Q57217688: Returning all desktop windows with PyWinAuto...\n",
      "  Collected Q64655079: Cannot import from pywinauto: ImportError: DLL loa...\n",
      "  Collected Q45800944: Getting window&#39;s properties using pywinauto...\n",
      "  Collected Q42213490: pywinauto: How to select this dialog? Which spying...\n",
      "  Collected Q28316597: pywinauto Wait and focus...\n",
      "  Collected Q12056590: How To Press &lt;CTRL&gt;...\n",
      "  Collected Q63638987: How to getting around pywinauto&#39;s ElementAmbig...\n",
      "  Collected Q63110628: How do I send keystrokes to a background window us...\n",
      "  Collected Q52305472: Importing pywinauto (or comtypes) clobbers existin...\n",
      "  Collected Q44307967: How do you control a right click spawned pop up me...\n",
      "  Collected Q40111006: Find children elements filter by type...\n",
      "  Collected Q50299472: pywin32 / pywinauto not working properly in remote...\n",
      "  Collected Q63760234: pywinauto: get URL from MS Edge Canary Address Bar...\n",
      "  Collected Q47018730: Pywinauto - How to wait on UIAWrapper windows...\n",
      "  Collected Q54951778: GUI automation using pywinauto python. Attribute e...\n",
      "  Collected Q41390461: print_control_identifiers() error in pywinauto...\n",
      "  Collected Q27544109: Error saving Firefox webpage using pywinauto in Py...\n",
      "  Collected Q52890720: How to click on TreeItem checkbox while Desktop is...\n",
      "  Collected Q52052558: How to click using pywinauto...\n",
      "  Collected Q41112765: pywinauto and listitem selection...\n",
      "  Collected Q50643931: How to wait for the specific image to appear on sc...\n",
      "  Collected Q45163039: Scrolling issues with pywinauto...\n",
      "  Collected Q54545463: In pywinauto, how can I right click my app icon in...\n",
      "  Collected Q46432544: Pywinauto how do I get the list of returned elemen...\n",
      "  Collected Q34745699: How to send SendKeys to Windows form in python scr...\n",
      "  Collected Q58446247: How to close a window using pywinauto which does n...\n",
      "  Collected Q58444877: Send keyboard/mouse inputs with python macro keep ...\n",
      "  Collected Q43871504: pywinauto find_elements() returns ElementNotFoundE...\n",
      "  Collected Q42917915: Pywinauto Windows Exists but not Visible...\n",
      "  Collected Q6376047: Is it possible to send data to a minimized window ...\n",
      "  Collected Q67657837: Can&#39;t send_keys to the background window. Erro...\n",
      "  Collected Q63515142: Pywinauto: How to set focus window on a program - ...\n",
      "  Collected Q54767813: pywinauto capture_as_image adds unwanted borders...\n",
      "  Collected Q46145284: How to get the current URL from Chrome using pywin...\n",
      "  Collected Q45140434: Find new window dialogs automatically...\n",
      "  Collected Q42516325: Unable to access elements of win32 application usi...\n",
      "  Collected Q40252343: Need a way to access control in a window with cont...\n",
      "  Collected Q39759376: Using pywinauto, how do I get Text of static text ...\n",
      "  Collected Q29697955: Pywinauto dont response...\n",
      "  Collected Q11719619: Looking for a way to use Pywinauto on Win x64...\n",
      "  Collected Q62037461: Getting error while running a script which uses py...\n",
      "  Collected Q61455024: While importing pywinauto in Tkinter window size c...\n",
      "  Collected Q59358742: Is there a way to speed up pywinauto?...\n",
      "  Collected Q59290028: Confirm PuTTY host key prompt with pywinauto...\n",
      "  Collected Q56873733: How to search children/descendants by regular expr...\n",
      "  Collected Q51824374: Cannot change content of an Edit control...\n",
      "  Collected Q40074054: Python Trouble getting cursor position (x, y) on d...\n",
      "  Collected Q40040564: Automating file upload using Selenium and pywinaut...\n",
      "  Collected Q33427735: How to get right click context menu in Windows app...\n",
      "  Collected Q33113913: pywinauto: MenuSelect() Cannot be used to select &...\n",
      "  Collected Q15607979: How to click a &#39;next&#39; button of a window u...\n",
      "  Collected Q78544920: pywinauto: how to get rid of delays in clicks...\n",
      "  Collected Q71116050: PywinAuto - Excel Automation Can not click the but...\n",
      "  Collected Q68454356: how can i expand or double click TreeItem without ...\n",
      "  Collected Q64655433: pywinauto emulate zoom with CTRL scroll does not w...\n",
      "  Collected Q56774363: How I can get the current focused element of deskt...\n",
      "  Collected Q54670224: How to click button in dialog box using PyWinAuto...\n",
      "  Collected Q53087532: Sending ALT-codes using pywinauto...\n",
      "  Collected Q51074956: Remote desktop connection using pywinauto...\n",
      "  Collected Q29765584: pywinauto keep remote desktop alive...\n",
      "  Collected Q29177243: Finding GUI elements with pywinauto...\n",
      "Saved 98 StackOverflow items to rag_data\\pywinauto\\stackoverflow\\pywinauto_stackoverflow.json\n",
      "\n",
      "[4/4] Combining all sources...\n",
      "\n",
      "Combined Data Summary:\n",
      "Total items: 369\n",
      "By source: {'documentation': 151, 'github': 120, 'stackoverflow': 98}\n",
      "Saved to: rag_data\\pywinauto\\combined\\pywinauto_combined.json\n",
      "\n",
      "================================================================================\n",
      "DATA COLLECTION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CELL 8: Usage Examples\n",
    "# ============================================================================\n",
    "\n",
    "# Example 1: Collect PyWinAuto data\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # PyWinAuto\n",
    "    pywinauto_df = collect_pywinauto_data(\n",
    "        github_token=os.environ.get(\"GITHUB_TOKEN\"),  # Optional but recommended\n",
    "        stackoverflow_key=os.environ.get(\"STACKOVWER_TOKEN\")  # Optional\n",
    "    )\n",
    "    \n",
    "    # Example 2: Collect data for another library (e.g., selenium)\n",
    "    # selenium_df = collect_library_data(\n",
    "    #     library_name='selenium',\n",
    "    #     docs_url='https://selenium-python.readthedocs.io/',\n",
    "    #     github_repo_url='https://github.com/SeleniumHQ/selenium',\n",
    "    #     stackoverflow_tag='selenium',\n",
    "    #     github_token='your_token',\n",
    "    #     stackoverflow_key='your_key'\n",
    "    # )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
