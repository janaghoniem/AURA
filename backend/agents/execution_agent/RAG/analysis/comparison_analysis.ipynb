{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ============================================================================\n",
    "# RECOMMENDED EMBEDDING MODELS FOR CODE DOCUMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "EMBEDDING_MODELS = {\n",
    "    # === BEST FOR CODE (RECOMMENDED) ===\n",
    "    \"codebert\": {\n",
    "        \"name\": \"microsoft/codebert-base\",\n",
    "        \"size_mb\": 500,\n",
    "        \"dims\": 768,\n",
    "        \"description\": \"Trained on code + NL, excellent for code docs\",\n",
    "        \"best_for\": \"Code-text hybrid content\"\n",
    "    },\n",
    "    \n",
    "    \"unixcoder\": {\n",
    "        \"name\": \"microsoft/unixcoder-base\",\n",
    "        \"size_mb\": 500,\n",
    "        \"dims\": 768,\n",
    "        \"description\": \"Cross-lingual code model, very good for technical docs\",\n",
    "        \"best_for\": \"API documentation and code snippets\"\n",
    "    },\n",
    "    \n",
    "    # === GENERAL PURPOSE (GOOD BALANCE) ===\n",
    "    \"bge-small\": {\n",
    "        \"name\": \"BAAI/bge-small-en-v1.5\",\n",
    "        \"size_mb\": 134,\n",
    "        \"dims\": 384,\n",
    "        \"description\": \"Fast, lightweight, good for general text\",\n",
    "        \"best_for\": \"Speed and memory efficiency\"\n",
    "    },\n",
    "    \n",
    "    \"bge-base\": {\n",
    "        \"name\": \"BAAI/bge-base-en-v1.5\",\n",
    "        \"size_mb\": 438,\n",
    "        \"dims\": 768,\n",
    "        \"description\": \"Balanced performance and speed\",\n",
    "        \"best_for\": \"Production use (best balance)\"\n",
    "    },\n",
    "    \n",
    "    \"bge-large\": {\n",
    "        \"name\": \"BAAI/bge-large-en-v1.5\",\n",
    "        \"size_mb\": 1340,\n",
    "        \"dims\": 1024,\n",
    "        \"description\": \"Highest quality, slower\",\n",
    "        \"best_for\": \"Maximum accuracy\"\n",
    "    },\n",
    "    \n",
    "    # === CURRENT DEFAULT ===\n",
    "    \"minilm\": {\n",
    "        \"name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"size_mb\": 90,\n",
    "        \"dims\": 384,\n",
    "        \"description\": \"Very fast, lightweight, your current model\",\n",
    "        \"best_for\": \"Baseline comparison\"\n",
    "    },\n",
    "    \n",
    "    # === SPECIALIZED ===\n",
    "    \"instructor\": {\n",
    "        \"name\": \"hkunlp/instructor-base\",\n",
    "        \"size_mb\": 440,\n",
    "        \"dims\": 768,\n",
    "        \"description\": \"Instruction-aware embeddings\",\n",
    "        \"best_for\": \"Task-specific embeddings\"\n",
    "    },\n",
    "    \n",
    "    \"e5-small\": {\n",
    "        \"name\": \"intfloat/e5-small-v2\",\n",
    "        \"size_mb\": 134,\n",
    "        \"dims\": 384,\n",
    "        \"description\": \"Microsoft's E5, strong performance\",\n",
    "        \"best_for\": \"Good general-purpose alternative\"\n",
    "    },\n",
    "    \n",
    "    \"e5-base\": {\n",
    "        \"name\": \"intfloat/e5-base-v2\",\n",
    "        \"size_mb\": 438,\n",
    "        \"dims\": 768,\n",
    "        \"description\": \"Microsoft's E5, better accuracy\",\n",
    "        \"best_for\": \"Higher quality than e5-small\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    \"\"\"Metrics for a single model\"\"\"\n",
    "    model_name: str\n",
    "    model_id: str\n",
    "    \n",
    "    # Performance metrics\n",
    "    mrr: float  # Mean Reciprocal Rank\n",
    "    recall_at_3: float\n",
    "    recall_at_5: float\n",
    "    ndcg_at_5: float  # Normalized Discounted Cumulative Gain\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    encoding_time_per_chunk: float  # seconds\n",
    "    search_time_per_query: float  # seconds\n",
    "    model_size_mb: float\n",
    "    embedding_dims: int\n",
    "    \n",
    "    # Quality metrics\n",
    "    avg_positive_similarity: float\n",
    "    avg_negative_similarity: float\n",
    "    separation_score: float  # positive_sim - negative_sim\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'model_id': self.model_id,\n",
    "            'mrr': f\"{self.mrr:.4f}\",\n",
    "            'recall@3': f\"{self.recall_at_3:.4f}\",\n",
    "            'recall@5': f\"{self.recall_at_5:.4f}\",\n",
    "            'ndcg@5': f\"{self.ndcg_at_5:.4f}\",\n",
    "            'encode_time_ms': f\"{self.encoding_time_per_chunk * 1000:.2f}\",\n",
    "            'search_time_ms': f\"{self.search_time_per_query * 1000:.2f}\",\n",
    "            'size_mb': f\"{self.model_size_mb:.1f}\",\n",
    "            'dims': self.embedding_dims,\n",
    "            'pos_sim': f\"{self.avg_positive_similarity:.4f}\",\n",
    "            'neg_sim': f\"{self.avg_negative_similarity:.4f}\",\n",
    "            'separation': f\"{self.separation_score:.4f}\"\n",
    "        }\n",
    "\n",
    "\n",
    "class EmbeddingModelComparison:\n",
    "    \"\"\"Compare multiple embedding models for code documentation\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks: List[Dict], test_queries: List[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize comparison framework\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of documentation chunks\n",
    "            test_queries: List of test queries with ground truth\n",
    "                         Format: [{\"query\": \"...\", \"relevant_ids\": [...]}, ...]\n",
    "        \"\"\"\n",
    "        self.chunks = chunks\n",
    "        self.test_queries = test_queries or self._generate_test_queries()\n",
    "        self.results = {}\n",
    "        \n",
    "    def _generate_test_queries(self) -> List[Dict]:\n",
    "        \"\"\"Generate test queries from chunks\"\"\"\n",
    "        print(\"ðŸ” Generating test queries from chunks...\")\n",
    "        \n",
    "        queries = []\n",
    "        \n",
    "        # Sample diverse chunks\n",
    "        sampled_chunks = np.random.choice(\n",
    "            self.chunks, \n",
    "            min(50, len(self.chunks)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for chunk in sampled_chunks:\n",
    "            # Create query from chunk context\n",
    "            context = chunk.get('context', '')\n",
    "            text = chunk.get('text', '')\n",
    "            chunk_id = chunk.get('metadata', {}).get('id', '')\n",
    "            \n",
    "            # Extract key phrases for query\n",
    "            if context:\n",
    "                # Use first sentence of context as query\n",
    "                query = context.split('.')[0].strip()\n",
    "                if len(query) > 10:  # Valid query\n",
    "                    queries.append({\n",
    "                        'query': query,\n",
    "                        'relevant_ids': [chunk_id],\n",
    "                        'source': 'context'\n",
    "                    })\n",
    "            \n",
    "            # Create queries from code examples\n",
    "            if 'code' in text.lower() or 'def ' in text:\n",
    "                # Extract function names or key terms\n",
    "                words = text.split()\n",
    "                if len(words) >= 5:\n",
    "                    query = ' '.join(words[:8])\n",
    "                    queries.append({\n",
    "                        'query': query,\n",
    "                        'relevant_ids': [chunk_id],\n",
    "                        'source': 'code'\n",
    "                    })\n",
    "        \n",
    "        print(f\"âœ… Generated {len(queries)} test queries\")\n",
    "        return queries[:30]  # Limit to 30 queries\n",
    "    \n",
    "    def evaluate_model(self, model_name: str, model_id: str) -> ModelMetrics:\n",
    "        \"\"\"\n",
    "        Evaluate a single embedding model\n",
    "        \n",
    "        Args:\n",
    "            model_name: Display name for the model\n",
    "            model_id: HuggingFace model ID\n",
    "            \n",
    "        Returns:\n",
    "            ModelMetrics with all evaluation scores\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EVALUATING: {model_name}\")\n",
    "        print(f\"Model ID: {model_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load model\n",
    "        print(\"ðŸ“¥ Loading model...\")\n",
    "        start_load = time.time()\n",
    "        try:\n",
    "            model = SentenceTransformer(model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to load model: {e}\")\n",
    "            return None\n",
    "        \n",
    "        load_time = time.time() - start_load\n",
    "        print(f\"âœ… Model loaded in {load_time:.2f}s\")\n",
    "        \n",
    "        # Get model info\n",
    "        embedding_dims = model.get_sentence_embedding_dimension()\n",
    "        model_size_mb = EMBEDDING_MODELS.get(model_name.lower(), {}).get('size_mb', 0)\n",
    "        \n",
    "        # Encode chunks\n",
    "        print(f\"\\nðŸ“Š Encoding {len(self.chunks)} chunks...\")\n",
    "        chunk_texts = [f\"{c['context']}\\n{c['text']}\" for c in self.chunks]\n",
    "        \n",
    "        start_encode = time.time()\n",
    "        chunk_embeddings = model.encode(\n",
    "            chunk_texts, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        encoding_time = time.time() - start_encode\n",
    "        encoding_time_per_chunk = encoding_time / len(self.chunks)\n",
    "        \n",
    "        print(f\"âœ… Encoding completed in {encoding_time:.2f}s\")\n",
    "        print(f\"   ({encoding_time_per_chunk*1000:.2f}ms per chunk)\")\n",
    "        \n",
    "        # Evaluate search quality\n",
    "        print(f\"\\nðŸ” Evaluating search quality with {len(self.test_queries)} queries...\")\n",
    "        \n",
    "        mrr_scores = []\n",
    "        recall_at_3_scores = []\n",
    "        recall_at_5_scores = []\n",
    "        ndcg_scores = []\n",
    "        search_times = []\n",
    "        \n",
    "        for query_data in self.test_queries:\n",
    "            query = query_data['query']\n",
    "            relevant_ids = set(query_data['relevant_ids'])\n",
    "            \n",
    "            # Encode query and search\n",
    "            start_search = time.time()\n",
    "            query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "            \n",
    "            # Compute similarities\n",
    "            similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "            top_indices = np.argsort(-similarities.numpy())[:5]\n",
    "            \n",
    "            search_time = time.time() - start_search\n",
    "            search_times.append(search_time)\n",
    "            \n",
    "            # Get top result IDs\n",
    "            top_ids = [\n",
    "                self.chunks[idx].get('metadata', {}).get('id', '')\n",
    "                for idx in top_indices\n",
    "            ]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            # MRR (Mean Reciprocal Rank)\n",
    "            for rank, chunk_id in enumerate(top_ids, 1):\n",
    "                if chunk_id in relevant_ids:\n",
    "                    mrr_scores.append(1.0 / rank)\n",
    "                    break\n",
    "            else:\n",
    "                mrr_scores.append(0.0)\n",
    "            \n",
    "            # Recall@3 and Recall@5\n",
    "            recall_at_3 = len(set(top_ids[:3]) & relevant_ids) / len(relevant_ids)\n",
    "            recall_at_5 = len(set(top_ids[:5]) & relevant_ids) / len(relevant_ids)\n",
    "            recall_at_3_scores.append(recall_at_3)\n",
    "            recall_at_5_scores.append(recall_at_5)\n",
    "            \n",
    "            # NDCG@5 (simplified)\n",
    "            dcg = sum([\n",
    "                1.0 / np.log2(rank + 1) \n",
    "                for rank, chunk_id in enumerate(top_ids, 1) \n",
    "                if chunk_id in relevant_ids\n",
    "            ])\n",
    "            idcg = sum([1.0 / np.log2(i + 1) for i in range(1, min(6, len(relevant_ids) + 1))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "            ndcg_scores.append(ndcg)\n",
    "        \n",
    "        # Evaluate embedding quality (similarity distribution)\n",
    "        print(f\"\\nðŸ“ˆ Evaluating embedding quality...\")\n",
    "        \n",
    "        # Sample pairs for similarity analysis\n",
    "        num_samples = min(100, len(self.chunks))\n",
    "        sampled_indices = np.random.choice(len(self.chunks), num_samples, replace=False)\n",
    "        sampled_embeddings = chunk_embeddings[sampled_indices]\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = util.cos_sim(sampled_embeddings, sampled_embeddings)\n",
    "        \n",
    "        # Calculate positive similarities (same document)\n",
    "        positive_sims = []\n",
    "        negative_sims = []\n",
    "        \n",
    "        for i in range(len(sampled_indices)):\n",
    "            for j in range(i + 1, len(sampled_indices)):\n",
    "                sim = similarity_matrix[i][j].item()\n",
    "                \n",
    "                # Check if same document\n",
    "                doc_i = self.chunks[sampled_indices[i]].get('metadata', {}).get('id', '')\n",
    "                doc_j = self.chunks[sampled_indices[j]].get('metadata', {}).get('id', '')\n",
    "                \n",
    "                if doc_i == doc_j and doc_i:\n",
    "                    positive_sims.append(sim)\n",
    "                else:\n",
    "                    negative_sims.append(sim)\n",
    "        \n",
    "        avg_positive_sim = np.mean(positive_sims) if positive_sims else 0.0\n",
    "        avg_negative_sim = np.mean(negative_sims) if negative_sims else 0.0\n",
    "        separation_score = avg_positive_sim - avg_negative_sim\n",
    "        \n",
    "        # Create metrics\n",
    "        metrics = ModelMetrics(\n",
    "            model_name=model_name,\n",
    "            model_id=model_id,\n",
    "            mrr=np.mean(mrr_scores),\n",
    "            recall_at_3=np.mean(recall_at_3_scores),\n",
    "            recall_at_5=np.mean(recall_at_5_scores),\n",
    "            ndcg_at_5=np.mean(ndcg_scores),\n",
    "            encoding_time_per_chunk=encoding_time_per_chunk,\n",
    "            search_time_per_query=np.mean(search_times),\n",
    "            model_size_mb=model_size_mb,\n",
    "            embedding_dims=embedding_dims,\n",
    "            avg_positive_similarity=avg_positive_sim,\n",
    "            avg_negative_similarity=avg_negative_sim,\n",
    "            separation_score=separation_score\n",
    "        )\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nðŸ“Š RESULTS FOR {model_name}:\")\n",
    "        print(f\"   MRR: {metrics.mrr:.4f}\")\n",
    "        print(f\"   Recall@3: {metrics.recall_at_3:.4f}\")\n",
    "        print(f\"   Recall@5: {metrics.recall_at_5:.4f}\")\n",
    "        print(f\"   NDCG@5: {metrics.ndcg_at_5:.4f}\")\n",
    "        print(f\"   Encoding: {metrics.encoding_time_per_chunk*1000:.2f}ms/chunk\")\n",
    "        print(f\"   Search: {metrics.search_time_per_query*1000:.2f}ms/query\")\n",
    "        print(f\"   Separation: {metrics.separation_score:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compare_models(self, model_list: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare multiple models\n",
    "        \n",
    "        Args:\n",
    "            model_list: List of model keys from EMBEDDING_MODELS\n",
    "                       If None, compares recommended models\n",
    "        \"\"\"\n",
    "        if model_list is None:\n",
    "            # Default: Compare best models for code\n",
    "            model_list = [\"bge-base\", \"bge-small\", \"minilm\", \"e5-base\"]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for model_key in model_list:\n",
    "            if model_key not in EMBEDDING_MODELS:\n",
    "                print(f\"âš ï¸ Unknown model: {model_key}\")\n",
    "                continue\n",
    "            \n",
    "            model_info = EMBEDDING_MODELS[model_key]\n",
    "            \n",
    "            try:\n",
    "                metrics = self.evaluate_model(\n",
    "                    model_name=model_key,\n",
    "                    model_id=model_info['name']\n",
    "                )\n",
    "                \n",
    "                if metrics:\n",
    "                    results.append(metrics.to_dict())\n",
    "                    self.results[model_key] = metrics\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error evaluating {model_key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Print comparison table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Save results\n",
    "        output_path = Path(\"embedding_comparison_results.json\")\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\nðŸ’¾ Results saved to {output_path}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_recommendation(self) -> str:\n",
    "        \"\"\"Get model recommendation based on results\"\"\"\n",
    "        if not self.results:\n",
    "            return \"No results available. Run compare_models() first.\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RECOMMENDATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Find best models for different criteria\n",
    "        best_accuracy = max(self.results.items(), key=lambda x: x[1].mrr)\n",
    "        best_speed = min(self.results.items(), key=lambda x: x[1].encoding_time_per_chunk)\n",
    "        best_balance = max(self.results.items(), \n",
    "                          key=lambda x: x[1].mrr / (x[1].encoding_time_per_chunk * 1000 + 1))\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ BEST ACCURACY: {best_accuracy[0]}\")\n",
    "        print(f\"   MRR: {best_accuracy[1].mrr:.4f}\")\n",
    "        print(f\"   Size: {best_accuracy[1].model_size_mb:.1f}MB\")\n",
    "        \n",
    "        print(f\"\\nâš¡ FASTEST: {best_speed[0]}\")\n",
    "        print(f\"   Speed: {best_speed[1].encoding_time_per_chunk*1000:.2f}ms/chunk\")\n",
    "        print(f\"   MRR: {best_speed[1].mrr:.4f}\")\n",
    "        \n",
    "        print(f\"\\nâš–ï¸  BEST BALANCE: {best_balance[0]}\")\n",
    "        print(f\"   MRR: {best_balance[1].mrr:.4f}\")\n",
    "        print(f\"   Speed: {best_balance[1].encoding_time_per_chunk*1000:.2f}ms/chunk\")\n",
    "        print(f\"   Size: {best_balance[1].model_size_mb:.1f}MB\")\n",
    "        \n",
    "        return best_balance[0]\n",
    "    \n",
    "    def save_best_model(self, criterion: str = \"balance\", output_dir: str = \"models/best\"):\n",
    "        \"\"\"\n",
    "        Save the best model based on specified criterion\n",
    "        \n",
    "        Args:\n",
    "            criterion: \"balance\", \"accuracy\", or \"speed\"\n",
    "            output_dir: Directory to save the best model\n",
    "            \n",
    "        Returns:\n",
    "            Path to saved model\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"âŒ No results available. Run compare_models() first.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"SAVING BEST MODEL (criterion: {criterion})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Select best model based on criterion\n",
    "        if criterion == \"accuracy\":\n",
    "            best_model_key, best_metrics = max(\n",
    "                self.results.items(), \n",
    "                key=lambda x: x[1].mrr\n",
    "            )\n",
    "            criterion_name = \"Accuracy (MRR)\"\n",
    "        elif criterion == \"speed\":\n",
    "            best_model_key, best_metrics = min(\n",
    "                self.results.items(), \n",
    "                key=lambda x: x[1].encoding_time_per_chunk\n",
    "            )\n",
    "            criterion_name = \"Speed\"\n",
    "        else:  # balance\n",
    "            best_model_key, best_metrics = max(\n",
    "                self.results.items(), \n",
    "                key=lambda x: x[1].mrr / (x[1].encoding_time_per_chunk * 1000 + 1)\n",
    "            )\n",
    "            criterion_name = \"Balance (MRR/Speed)\"\n",
    "        \n",
    "        print(f\"\\nðŸ† BEST MODEL BY {criterion_name}: {best_model_key}\")\n",
    "        print(f\"   Model ID: {best_metrics.model_id}\")\n",
    "        print(f\"   MRR: {best_metrics.mrr:.4f}\")\n",
    "        print(f\"   Speed: {best_metrics.encoding_time_per_chunk*1000:.2f}ms/chunk\")\n",
    "        print(f\"   Size: {best_metrics.model_size_mb:.1f}MB\")\n",
    "        print(f\"   Dimensions: {best_metrics.embedding_dims}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Load and save the best model\n",
    "        print(f\"\\nðŸ“¥ Loading best model...\")\n",
    "        try:\n",
    "            best_model = SentenceTransformer(best_metrics.model_id)\n",
    "            \n",
    "            # Save model\n",
    "            model_save_path = output_path / \"embedding_model\"\n",
    "            best_model.save(str(model_save_path))\n",
    "            print(f\"âœ… Model saved to: {model_save_path}\")\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                \"model_name\": best_model_key,\n",
    "                \"model_id\": best_metrics.model_id,\n",
    "                \"selection_criterion\": criterion,\n",
    "                \"metrics\": best_metrics.to_dict(),\n",
    "                \"description\": EMBEDDING_MODELS.get(best_model_key, {}).get('description', ''),\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            metadata_path = output_path / \"model_info.json\"\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            print(f\"âœ… Metadata saved to: {metadata_path}\")\n",
    "            \n",
    "            # Save comparison results\n",
    "            comparison_path = output_path / \"comparison_results.json\"\n",
    "            all_results = [metrics.to_dict() for metrics in self.results.values()]\n",
    "            with open(comparison_path, 'w') as f:\n",
    "                json.dump(all_results, f, indent=2)\n",
    "            print(f\"âœ… Comparison results saved to: {comparison_path}\")\n",
    "            \n",
    "            print(f\"\\n\" + \"=\"*80)\n",
    "            print(f\"âœ… BEST MODEL SAVED SUCCESSFULLY\")\n",
    "            print(f\"=\"*80)\n",
    "            print(f\"\\nðŸ“‚ Output directory: {output_path.absolute()}\")\n",
    "            print(f\"\\nTo use this model in your RAG system:\")\n",
    "            print(f\"   config.base_model = '{best_metrics.model_id}'\")\n",
    "            print(f\"\\nOr load from saved path:\")\n",
    "            print(f\"   model = SentenceTransformer('{model_save_path.absolute()}')\")\n",
    "            \n",
    "            return model_save_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def run_model_comparison(chunks: List[Dict], save_best: bool = True):\n",
    "    \"\"\"\n",
    "    Run complete model comparison and optionally save best model\n",
    "    \n",
    "    Args:\n",
    "        chunks: Your preprocessed documentation chunks\n",
    "        save_best: Whether to automatically save the best model\n",
    "    \"\"\"\n",
    "    # Initialize comparison\n",
    "    comparison = EmbeddingModelComparison(chunks)\n",
    "    \n",
    "    # Compare recommended models for code documentation\n",
    "    print(\"\\nðŸš€ Starting Model Comparison...\")\n",
    "    print(f\"   Testing with {len(chunks)} chunks\")\n",
    "    print(f\"   Using {len(comparison.test_queries)} test queries\")\n",
    "    \n",
    "    # Option 1: Compare specific models\n",
    "    models_to_test = [\n",
    "        # \"codebert\",\n",
    "        # \"unixcoder\" \n",
    "        # \"bge-base\",      # Best general-purpose\n",
    "        # \"bge-small\",     # Fast and efficient\n",
    "        \"minilm\"        # Your current baseline\n",
    "        # \"e5-base\",       # Strong alternative\n",
    "        # \"codebert\",    # Uncomment if testing code-specific models\n",
    "    ]\n",
    "    \n",
    "    results_df = comparison.compare_models(models_to_test)\n",
    "    \n",
    "    # Get recommendation\n",
    "    best_model = comparison.get_recommendation()\n",
    "    \n",
    "    print(f\"\\nâœ… RECOMMENDED MODEL: {best_model}\")\n",
    "    print(f\"\\nðŸ’¡ To use this model, update your EmbeddingConfig:\")\n",
    "    print(f\"   base_model = '{EMBEDDING_MODELS[best_model]['name']}'\")\n",
    "    \n",
    "    # Save best model if requested\n",
    "    if save_best:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SAVING BEST MODELS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Save all three variants\n",
    "        print(\"\\nðŸ“¦ Saving best model by different criteria...\")\n",
    "        \n",
    "        # 1. Best balance (recommended)\n",
    "        balance_path = comparison.save_best_model(\n",
    "            criterion=\"balance\", \n",
    "            output_dir=\"models/best\"\n",
    "        )\n",
    "        \n",
    "        # 2. Best accuracy\n",
    "        accuracy_path = comparison.save_best_model(\n",
    "            criterion=\"accuracy\",\n",
    "            output_dir=\"models/best_accuracy\"\n",
    "        )\n",
    "        \n",
    "        # 3. Best speed\n",
    "        speed_path = comparison.save_best_model(\n",
    "            criterion=\"speed\",\n",
    "            output_dir=\"models/best_speed\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SAVED MODEL PATHS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nâš–ï¸  Best Balance: {balance_path}\")\n",
    "        print(f\"ðŸŽ¯ Best Accuracy: {accuracy_path}\")\n",
    "        print(f\"âš¡ Best Speed: {speed_path}\")\n",
    "        \n",
    "        print(\"\\nðŸ’¡ Recommended for production: Use 'models/best'\")\n",
    "    \n",
    "    return comparison, results_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STANDALONE FUNCTION TO SAVE BEST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def save_comparison_best_model(comparison: EmbeddingModelComparison, \n",
    "                               criterion: str = \"balance\",\n",
    "                               output_dir: str = \"models/best\"):\n",
    "    \"\"\"\n",
    "    Standalone function to save best model after comparison\n",
    "    \n",
    "    Args:\n",
    "        comparison: EmbeddingModelComparison instance with results\n",
    "        criterion: \"balance\", \"accuracy\", or \"speed\"\n",
    "        output_dir: Directory to save model\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved model\n",
    "    \n",
    "    Example:\n",
    "        comparison, results = run_model_comparison(chunks, save_best=False)\n",
    "        \n",
    "        # Later, save specific model\n",
    "        save_comparison_best_model(comparison, criterion=\"accuracy\", \n",
    "                                   output_dir=\"models/best_accuracy\")\n",
    "    \"\"\"\n",
    "    return comparison.save_best_model(criterion=criterion, output_dir=output_dir)\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     config = EmbeddingConfig(\"pyautogui\")\n",
    "    \n",
    "#     # Step 1: Load and preprocess data\n",
    "#     print(\"\\n[STEP 1/5] Loading and preprocessing data...\")\n",
    "#     loader = RAGDataLoader(config)\n",
    "#     df = loader.load_data()\n",
    "#     chunks = loader.prepare_all_chunks()\n",
    "#     # Load your chunks\n",
    "#     # chunks = loader.prepare_all_chunks()\n",
    "    \n",
    "#     # Option 1: Run comparison and auto-save best model\n",
    "#     comparison, results = run_model_comparison(chunks, save_best=True)\n",
    "    \n",
    "#     # Option 2: Run comparison without saving, then save later\n",
    "#     # comparison, results = run_model_comparison(chunks, save_best=False)\n",
    "#     # \n",
    "#     # # Review results, then save manually\n",
    "#     # save_comparison_best_model(comparison, criterion=\"balance\", output_dir=\"models/best\")\n",
    "#     # save_comparison_best_model(comparison, criterion=\"accuracy\", output_dir=\"models/best_accuracy\")\n",
    "#     # save_comparison_best_model(comparison, criterion=\"speed\", output_dir=\"models/best_speed\")\n",
    "    \n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    config = EmbeddingConfig(\"pyautogui\")\n",
    "    \n",
    "    # Step 1: Load and preprocess data\n",
    "    print(\"\\n[STEP 1/5] Loading and preprocessing data...\")\n",
    "    loader = RAGDataLoader(config)\n",
    "    df = loader.load_data()\n",
    "    chunks = loader.prepare_all_chunks()\n",
    "    # Load your chunks\n",
    "    # chunks = loader.prepare_all_chunks()\n",
    "    \n",
    "    # Option 1: Run comparison and auto-save best model\n",
    "    comparison, results = run_model_comparison(chunks, save_best=False)\n",
    "    \n",
    "    # Option 2: Run comparison without saving, then save later\n",
    "    # comparison, results = run_model_comparison(chunks, save_best=False)\n",
    "    # \n",
    "    # # Review results, then save manually\n",
    "    save_comparison_best_model(comparison, criterion=\"balance\", output_dir=\"models/best\")\n",
    "    # save_comparison_best_model(comparison, criterion=\"accuracy\", output_dir=\"models/best_accuracy\")\n",
    "    # save_comparison_best_model(comparison, criterion=\"speed\", output_dir=\"models/best_speed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Add Top-K Analysis Function (NEW CODE TO ADD)\n",
    "# ======================================================\n",
    "\n",
    "def analyze_optimal_topk_for_best_model(comparison: EmbeddingModelComparison, \n",
    "                                        chunks: List[Dict],\n",
    "                                        best_model_name: str = None,\n",
    "                                        k_values: List[int] = None):\n",
    "    \"\"\"\n",
    "    Analyze optimal top-k for the best performing model\n",
    "    \n",
    "    Args:\n",
    "        comparison: Your existing EmbeddingModelComparison object\n",
    "        chunks: Your chunk data\n",
    "        best_model_name: Name of best model (if None, uses best from comparison)\n",
    "        k_values: K values to test (default: [1,2,3,5,7,10])\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with optimal k and detailed results\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if k_values is None:\n",
    "        k_values = [1, 2, 3, 5, 7, 10]\n",
    "    \n",
    "    # Step 2.1: Identify best model\n",
    "    if best_model_name is None:\n",
    "        # Get best model from comparison results\n",
    "        best_model_name = max(\n",
    "            comparison.results.items(),\n",
    "            key=lambda x: x[1].mrr\n",
    "        )[0]\n",
    "    \n",
    "    best_metrics = comparison.results[best_model_name]\n",
    "    model_id = best_metrics.model_id\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"TOP-K ANALYSIS FOR BEST MODEL: {best_model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    print(f\"Current MRR: {best_metrics.mrr:.4f}\")\n",
    "    print(f\"Testing K values: {k_values}\")\n",
    "    \n",
    "    # Step 2.2: Load model and encode chunks\n",
    "    print(\"\\nðŸ“¥ Loading model...\")\n",
    "    model = SentenceTransformer(model_id)\n",
    "    \n",
    "    print(\"ðŸ“Š Encoding chunks...\")\n",
    "    chunk_texts = [f\"{c['context']}\\n{c['text']}\" for c in chunks]\n",
    "    chunk_embeddings = model.encode(\n",
    "        chunk_texts,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    # Step 2.3: Test each k value\n",
    "    print(f\"\\nðŸ” Testing {len(comparison.test_queries)} queries at different K values...\")\n",
    "    \n",
    "    recall_at_k = {k: [] for k in k_values}\n",
    "    precision_at_k = {k: [] for k in k_values}\n",
    "    \n",
    "    for query_data in comparison.test_queries:\n",
    "        query = query_data['query']\n",
    "        relevant_ids = set(query_data['relevant_ids'])\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "        \n",
    "        # Get similarities and sort\n",
    "        similarities = util.cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "        top_indices = np.argsort(-similarities.numpy())\n",
    "        \n",
    "        # Get all chunk IDs in order\n",
    "        all_chunk_ids = [\n",
    "            chunks[idx].get('metadata', {}).get('id', '')\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        # Calculate metrics at each k\n",
    "        for k in k_values:\n",
    "            top_k_ids = set(all_chunk_ids[:k])\n",
    "            \n",
    "            # Recall: What % of relevant docs found?\n",
    "            recall = len(top_k_ids & relevant_ids) / len(relevant_ids) if relevant_ids else 0\n",
    "            recall_at_k[k].append(recall)\n",
    "            \n",
    "            # Precision: What % of retrieved docs relevant?\n",
    "            precision = len(top_k_ids & relevant_ids) / k if k > 0 else 0\n",
    "            precision_at_k[k].append(precision)\n",
    "    \n",
    "    # Step 2.4: Calculate averages\n",
    "    avg_recall = {k: np.mean(scores) for k, scores in recall_at_k.items()}\n",
    "    avg_precision = {k: np.mean(scores) for k, scores in precision_at_k.items()}\n",
    "    \n",
    "    # Step 2.5: Print results table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS: RECALL & PRECISION AT DIFFERENT K VALUES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'K':<6} {'Recall':<12} {'Precision':<12} {'Gain from Prev':<18} {'Cost':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    prev_recall = 0\n",
    "    for i, k in enumerate(k_values):\n",
    "        recall = avg_recall[k]\n",
    "        precision = avg_precision[k]\n",
    "        gain = recall - prev_recall\n",
    "        extra_chunks = k - (k_values[i-1] if i > 0 else 0)\n",
    "        \n",
    "        # Format with color coding\n",
    "        gain_str = f\"+{gain:.4f} ({gain*100:+.1f}%)\"\n",
    "        cost_str = f\"+{extra_chunks} chunks\" if i > 0 else f\"{k} chunks\"\n",
    "        \n",
    "        print(f\"{k:<6} {recall:<12.4f} {precision:<12.4f} {gain_str:<18} {cost_str:<10}\")\n",
    "        prev_recall = recall\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 2.6: Analyze diminishing returns\n",
    "    print(\"\\nðŸ’¡ DIMINISHING RETURNS ANALYSIS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i in range(1, len(k_values)):\n",
    "        k_prev = k_values[i-1]\n",
    "        k_curr = k_values[i]\n",
    "        \n",
    "        gain = avg_recall[k_curr] - avg_recall[k_prev]\n",
    "        extra_chunks = k_curr - k_prev\n",
    "        gain_per_chunk = gain / extra_chunks\n",
    "        \n",
    "        # Color code by efficiency\n",
    "        if gain > 0.05:\n",
    "            status = \"âœ… GOOD GAIN\"\n",
    "        elif gain > 0.02:\n",
    "            status = \"âš ï¸  MODERATE GAIN\"\n",
    "        else:\n",
    "            status = \"âŒ DIMINISHING RETURNS\"\n",
    "        \n",
    "        print(f\"K={k_prev}â†’{k_curr}: \"\n",
    "              f\"Gain: {gain:.4f} (+{gain*100:.1f}%) | \"\n",
    "              f\"Cost: +{extra_chunks} chunks | \"\n",
    "              f\"Efficiency: {gain_per_chunk:.4f}/chunk | \"\n",
    "              f\"{status}\")\n",
    "    \n",
    "    # Step 2.7: Make recommendation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find optimal k (where we hit 90% recall or diminishing returns kick in)\n",
    "    optimal_k = k_values[0]\n",
    "    \n",
    "    # Strategy 1: First k that achieves 90% recall\n",
    "    for k in k_values:\n",
    "        if avg_recall[k] >= 0.90:\n",
    "            optimal_k = k\n",
    "            break\n",
    "    \n",
    "    # Strategy 2: If no 90% recall, use diminishing returns\n",
    "    if avg_recall[optimal_k] < 0.90:\n",
    "        for i in range(1, len(k_values)):\n",
    "            k_curr = k_values[i]\n",
    "            k_prev = k_values[i-1]\n",
    "            gain = avg_recall[k_curr] - avg_recall[k_prev]\n",
    "            \n",
    "            if gain < 0.03:  # Less than 3% gain\n",
    "                optimal_k = k_prev\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ OPTIMAL K: {optimal_k}\")\n",
    "    print(f\"\\nðŸ“Š Performance at K={optimal_k}:\")\n",
    "    print(f\"   â€¢ Recall: {avg_recall[optimal_k]:.4f} ({avg_recall[optimal_k]*100:.1f}%)\")\n",
    "    print(f\"   â€¢ Precision: {avg_precision[optimal_k]:.4f} ({avg_precision[optimal_k]*100:.1f}%)\")\n",
    "    print(f\"   â€¢ Retrieves {optimal_k} chunks per query\")\n",
    "    \n",
    "    print(f\"\\nðŸ’­ Why K={optimal_k}?\")\n",
    "    if avg_recall[optimal_k] >= 0.90:\n",
    "        print(f\"   âœ… Achieves 90%+ recall ({avg_recall[optimal_k]*100:.1f}%)\")\n",
    "    \n",
    "    # Check next k\n",
    "    next_k_idx = k_values.index(optimal_k) + 1\n",
    "    if next_k_idx < len(k_values):\n",
    "        next_k = k_values[next_k_idx]\n",
    "        gain_to_next = avg_recall[next_k] - avg_recall[optimal_k]\n",
    "        extra_chunks = next_k - optimal_k\n",
    "        print(f\"   ðŸ“‰ Going to K={next_k} only gains {gain_to_next*100:.1f}% \"\n",
    "              f\"for {extra_chunks} more chunks (not worth it)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ UPDATE YOUR RAG CONFIG:\")\n",
    "    print(f\"   config.top_k = {optimal_k}\")\n",
    "    \n",
    "    # Step 2.8: Plot results\n",
    "    plot_topk_results(k_values, avg_recall, avg_precision, \n",
    "                     best_model_name, optimal_k)\n",
    "    \n",
    "    # Step 2.9: Save results\n",
    "    results = {\n",
    "        'model_name': best_model_name,\n",
    "        'model_id': model_id,\n",
    "        'optimal_k': optimal_k,\n",
    "        'k_values': k_values,\n",
    "        'recall_at_k': avg_recall,\n",
    "        'precision_at_k': avg_precision\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('optimal_topk_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nðŸ’¾ Results saved to: optimal_topk_results.json\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_topk_results(k_values, recall, precision, model_name, optimal_k):\n",
    "    \"\"\"Plot recall and precision curves with optimal k marked\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Plot recall\n",
    "    recall_values = [recall[k] for k in k_values]\n",
    "    ax.plot(k_values, recall_values, marker='o', linewidth=2.5, \n",
    "            markersize=10, label='Recall', color='#2E86AB')\n",
    "    \n",
    "    # Plot precision\n",
    "    precision_values = [precision[k] for k in k_values]\n",
    "    ax.plot(k_values, precision_values, marker='s', linewidth=2.5,\n",
    "            markersize=10, label='Precision', color='#A23B72')\n",
    "    \n",
    "    # Mark optimal k\n",
    "    ax.axvline(x=optimal_k, color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Optimal K={optimal_k}')\n",
    "    ax.axhline(y=0.90, color='red', linestyle=':', \n",
    "               linewidth=1.5, alpha=0.7, label='90% Target')\n",
    "    \n",
    "    # Add value labels\n",
    "    for k, r in zip(k_values, recall_values):\n",
    "        ax.annotate(f'{r:.3f}', xy=(k, r), xytext=(0, 10),\n",
    "                   textcoords='offset points', ha='center', \n",
    "                   fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('K (Number of Retrieved Chunks)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(f'Top-K Analysis: {model_name}', fontsize=15, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(fontsize=11, loc='lower right')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xticks(k_values)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'topk_analysis_{model_name}.png'\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nðŸ“Š Plot saved to: {filename}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: RUN THE TOP-K ANALYSIS (ADD THIS TO YOUR CODE)\n",
    "# ============================================================================\n",
    "\n",
    "# After you run your model comparison:\n",
    "# comparison, results_df = run_model_comparison(chunks, save_best=True)\n",
    "\n",
    "# NOW ADD THIS:\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: ANALYZING OPTIMAL TOP-K FOR BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run top-k analysis on best model\n",
    "topk_results = analyze_optimal_topk_for_best_model(\n",
    "    comparison=comparison,\n",
    "    chunks=chunks,\n",
    "    best_model_name=None,  # Auto-select best model\n",
    "    k_values=[1, 2, 3, 5, 7, 10, 15]  # Test these k values\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ANALYSIS COMPLETE!\")\n",
    "print(f\"ðŸŽ¯ Optimal K for your RAG system: {topk_results['optimal_k']}\")\n",
    "print(f\"ðŸ“Š Recall at K={topk_results['optimal_k']}: \"\n",
    "      f\"{topk_results['recall_at_k'][topk_results['optimal_k']]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE EXAMPLE: Full Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def complete_embedding_optimization_pipeline(chunks: List[Dict]):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Compare models â†’ Find best â†’ Optimize top-k\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"ðŸš€\"*40)\n",
    "    print(\"COMPLETE EMBEDDING OPTIMIZATION PIPELINE\")\n",
    "    print(\"ðŸš€\"*40)\n",
    "    \n",
    "    # Step 1: Compare embedding models\n",
    "    print(\"\\nðŸ“Š STEP 1: Comparing embedding models...\")\n",
    "    comparison, results_df = run_model_comparison(chunks, save_best=True)\n",
    "    \n",
    "    # Step 2: Find optimal top-k for best model\n",
    "    print(\"\\nðŸ“Š STEP 2: Finding optimal top-k...\")\n",
    "    topk_results = analyze_optimal_topk_for_best_model(\n",
    "        comparison=comparison,\n",
    "        chunks=chunks,\n",
    "        k_values=[1, 2, 3, 5, 7, 10, 15]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Print final recommendations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RECOMMENDATIONS FOR YOUR RAG SYSTEM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_model = topk_results['model_name']\n",
    "    optimal_k = topk_results['optimal_k']\n",
    "    \n",
    "    print(f\"\\nðŸ† Best Embedding Model: {best_model}\")\n",
    "    print(f\"   Model ID: {topk_results['model_id']}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Optimal Top-K: {optimal_k}\")\n",
    "    print(f\"   Recall: {topk_results['recall_at_k'][optimal_k]:.4f}\")\n",
    "    print(f\"   Precision: {topk_results['precision_at_k'][optimal_k]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ UPDATE YOUR CONFIG:\")\n",
    "    print(f\"   class RAGConfig:\")\n",
    "    print(f\"       base_model = '{topk_results['model_id']}'\")\n",
    "    print(f\"       top_k = {optimal_k}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Files Generated:\")\n",
    "    print(f\"   â€¢ models/best/embedding_model/\")\n",
    "    print(f\"   â€¢ optimal_topk_results.json\")\n",
    "    print(f\"   â€¢ topk_analysis_{best_model}.png\")\n",
    "    print(f\"   â€¢ embedding_comparison_results.json\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'optimal_k': optimal_k,\n",
    "        'comparison': comparison,\n",
    "        'topk_results': topk_results\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HOW TO USE IN YOUR CODE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your chunks\n",
    "    # âœ… ADD THIS:\n",
    "    topk_results = analyze_optimal_topk_for_best_model(\n",
    "        comparison=comparison,\n",
    "        chunks=chunks,\n",
    "        k_values=[1, 2, 3, 5, 7, 10, 15]\n",
    "    )\n",
    "\n",
    "print(f\"ðŸŽ¯ Optimal K: {topk_results['optimal_k']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
