{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643f45ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\hala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hala\\.cache\\huggingface\\hub\\datasets--2077AIDataFoundation--VeriGUI. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating test split: 302 examples [00:00, 3233.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VeriGUI with 302 tasks (split='test').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folder': 'V3_33',\n",
       " 'instruct': \"Count the cumulative number of visitors to Asia's largest theme parks from 2015 to 2024 (if data for 2024 is unavailable, collect data for available years only), identify the park with the highest visitor count, and list the park's name, city, total visitor count, year with the highest annual visitor count, name and opening date of the signature new attraction introduced that year, price increase for single-day tickets for ages 12 and above (excluding price changes due to special circumstances, with specific ticket price changes based on local price adjustments), operating company, and largest shareholder.\",\n",
       " 'result': ['visitor attendance data of major theme parks in Asia from 2015 to 2024 ,  Tokyo Disneyland 2015: 16,600,000 2016: 16,540,000 2017: 16,600,000 2018: 17,907,000 2019: 17,910,000 2020: 4,160,000 2021: 6,300,000 2022: 12,000,000 2023: 15,100,000  Tokyo DisneySea 2015: 12,100,000 2016: 13,460,000 2017: 13,500,000 2018: 14,651,000 2019: 14,650,000 2020: 3,400,000 2021: 5,800,000 2022: 10,100,000 2023: 12,400,000  Universal Studios Japan 2015: 13,900,000 2016: 14,500,000 2017: 14,935,000 2018: 14,300,000 2019: 14,500,000 2020: 4,901,000 2021: 5,500,000 2022: 12,350,000 2023: 16,000,000  Shanghai Disneyland 2016: 5,600,000 2017: 11,000,000 2018: 11,800,000 2019: 11,210,000 2020: 5,500,000 2021: 8,480,000 2022: 5,300,000 2023: 14,000,000  Chimelong Ocean Kingdom 2015: 7,486,000 2016: 8,474,000 2017: 9,788,000 2018: 10,830,000 2019: 11,736,000 2020: 4,797,000 2021: 7,452,000 2022: 4,400,000 2023: 12,520,000  Hong Kong Disneyland 2015: 6,800,000 2016: 6,100,000 2017: 6,200,000 2018: 6,700,000 2019: 5,695,000 2020: 1,700,000 2021: 2,800,000 2022: 3,400,000 2023: 6,400,000  Universal Studios Singapore 2015: 4,200,000 2016: 4,100,000 2017: 4,220,000 2018: 4,400,000 2019: 4,500,000 2020: 1,098,000 2021: 1,200,000 2022: 2,100,000  Everland 2015: 7,423,000 2016: 7,200,000 2017: 6,310,000 2018: 5,850,000  Universal Studios Beijing 2022: 4,300,000 2023: 9,000,000',\n",
       "  'the cumulative number of visitors for each park: Tokyo Disneyland: 123,117,000 Tokyo DisneySea: 111,948,000 Universal Studios Japan:131,946,000 Shanghai Disneyland\\tShanghai: 77,890,000 Chimelong Ocean Kingdom: 77,497,000 Hong Kong Disneyland: 45,095,000 Universal Studios Singapore: 25,818,000 Everland:  26,783,000  Universal Studios Beijing: 13,300,000 ',\n",
       "  'the park with the highest cumulative attendance and its city: Universal Studios Japan â\\x80\\x93 Osaka, Japan. Total Attendance: 131,946,000 visitors; the year when this park had the highest annual attendance and the visitor number for that year: 2023, 16,000,000.',\n",
       "  'name of the featured new attraction: Doraemon XR Ride: Nobitaâ\\x80\\x99s Sky Utopia opening time: February 23, 2023',\n",
       "  'the changes in ticket prices:  2015, Â¥7,200;   2016\\t, Â¥7,400;  2017, Â¥7,600;  2018, Â¥7,900;  2019, Â¥8,200;  2020, Â¥7,800;  2021, Â¥8200;  2022,  Â¥8,400;  2023,  Â¥8,600;   2024\\t,  Â¥8,600;  2025, Â¥8,600;  ',\n",
       "  'the price increase margin: 19.44%',\n",
       "  'operating companyï¼\\x9a USJ LLC largest shareholderï¼\\x9a Comcast NBCUniversal'],\n",
       " 'sub_tasks': [{'instruct': 'Collect the annual visitor attendance data of major theme parks in Asia from 2015 to 2024.',\n",
       "   'result': ['Tokyo Disneyland 2015: 16,600,000 2016: 16,540,000 2017: 16,600,000 2018: 17,907,000 2019: 17,910,000 2020: 4,160,000 2021: 6,300,000 2022: 12,000,000 2023: 15,100,000  Tokyo DisneySea 2015: 12,100,000 2016: 13,460,000 2017: 13,500,000 2018: 14,651,000 2019: 14,650,000 2020: 3,400,000 2021: 5,800,000 2022: 10,100,000 2023: 12,400,000  Universal Studios Japan 2015: 13,900,000 2016: 14,500,000 2017: 14,935,000 2018: 14,300,000 2019: 14,500,000 2020: 4,901,000 2021: 5,500,000 2022: 12,350,000 2023: 16,000,000  Shanghai Disneyland 2016: 5,600,000 2017: 11,000,000 2018: 11,800,000 2019: 11,210,000 2020: 5,500,000 2021: 8,480,000 2022: 5,300,000 2023: 14,000,000  Chimelong Ocean Kingdom 2015: 7,486,000 2016: 8,474,000 2017: 9,788,000 2018: 10,830,000 2019: 11,736,000 2020: 4,797,000 2021: 7,452,000 2022: 4,400,000 2023: 12,520,000  Hong Kong Disneyland 2015: 6,800,000 2016: 6,100,000 2017: 6,200,000 2018: 6,700,000 2019: 5,695,000 2020: 1,700,000 2021: 2,800,000 2022: 3,400,000 2023: 6,400,000  Universal Studios Singapore 2015: 4,200,000 2016: 4,100,000 2017: 4,220,000 2018: 4,400,000 2019: 4,500,000 2020: 1,098,000 2021: 1,200,000 2022: 2,100,000  Everland 2015: 7,423,000 2016: 7,200,000 2017: 6,310,000 2018: 5,850,000  Universal Studios Beijing 2022: 4,300,000 2023: 9,000,000']},\n",
       "  {'instruct': 'Using the annual visitor attendance data of major theme parks in Asia from 2015 to 2024, calculate the cumulative number of visitors for each park and identify the park with the highest cumulative attendance and its city.',\n",
       "   'result': ['the cumulative number of visitors for each park: Tokyo Disneyland: 123,117,000 Tokyo DisneySea: 111,948,000 Universal Studios Japan:131,946,000 Shanghai Disneyland\\tShanghai: 77,890,000 Chimelong Ocean Kingdom: 77,497,000 Hong Kong Disneyland: 45,095,000 Universal Studios Singapore: 25,818,000 Everland:  26,783,000  Universal Studios Beijing: 13,300,000 ',\n",
       "    'the park with the highest cumulative attendance and its city: Universal Studios Japan â\\x80\\x93 Osaka, Japan',\n",
       "    'Total Attendance: 131,946,000 visitors']},\n",
       "  {'instruct': 'For Universal Studios Japan in Osaka, Japan (the park with the highest cumulative attendance), find the year when it had the highest annual attendance and record the visitor number for that year.',\n",
       "   'result': ['the year when this park had the highest annual attendance and the visitor number for that year: 2023, 16,000,000.']},\n",
       "  {'instruct': 'For Universal Studios Japan in Osaka, Japan, identify the name and opening time of the featured new attraction launched in 2023, the year with the highest annual attendance.',\n",
       "   'result': ['name of the featured new attraction: Doraemon XR Ride: Nobitaâ\\x80\\x99s Sky Utopia',\n",
       "    'opening time: February 23, 2023']},\n",
       "  {'instruct': 'Collect the changes in ticket prices of Universal Studios Japan in Osaka, Japan from 2015 to 2024 and calculate the price increase margin.',\n",
       "   'result': ['the changes in ticket prices:  2015, Â¥7,200;   2016\\t, Â¥7,400;  2017, Â¥7,600;  2018, Â¥7,400;  2019, Â¥4,100;  2020, Â¥4,200;  2021, Â¥8200;  2022,  Â¥8,400;  2023,  Â¥8,600;   2024\\t,  Â¥8,600;  2025, Â¥8,600;  ',\n",
       "    'the price increase margin: 19.44%']},\n",
       "  {'instruct': 'Find the operating company of Universal Studios Japan in Osaka, Japan and the name of its largest shareholder.',\n",
       "   'result': ['operating companyï¼\\x9a USJ LLC',\n",
       "    'largest shareholderï¼\\x9a Comcast NBCUniversal']}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_verigui():\n",
    "    ds_dict = load_dataset(\"2077AIDataFoundation/VeriGUI\")\n",
    "    # Use the only available split: \"test\"\n",
    "    ds = ds_dict[\"test\"]\n",
    "    print(f\"Loaded VeriGUI with {len(ds)} tasks (split='test').\")\n",
    "    return ds\n",
    "\n",
    "ds = load_verigui()\n",
    "ds[0]  # preview first example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65a85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map agent names to integer IDs\n",
    "# AGENT_IDS = {\"nav_agent\": 0, \"pers_agent\": 1, \"llm_agent\": 2}\n",
    "\n",
    "# def infer_agent_for_subtask(sub_instruction):\n",
    "#     \"\"\"Heuristic agent assignment based on subtask instruction\"\"\"\n",
    "#     inst = sub_instruction.lower()\n",
    "#     if any(k in inst for k in ['search', 'find', 'look up', 'navigate', 'click', 'scroll']):\n",
    "#         return 'nav_agent'\n",
    "#     if any(k in inst for k in ['filter', 'analyze', 'compare', 'extract', 'identify', 'plan']):\n",
    "#         return 'llm_agent'\n",
    "#     if any(k in inst for k in ['suggest', 'recommend', 'adapt', 'personalize']):\n",
    "#         return 'pers_agent'\n",
    "#     # default alternating\n",
    "#     return 'llm_agent'\n",
    "\n",
    "# def create_numeric_state(task_instruction, completed_subtasks, current_subtask, step_idx, total_steps):\n",
    "#     \"\"\"Create a numeric state vector\"\"\"\n",
    "#     state = []\n",
    "\n",
    "#     # Progress\n",
    "#     progress = step_idx / total_steps if total_steps > 0 else 0.0\n",
    "#     state.append(progress)\n",
    "\n",
    "#     # Task complexity\n",
    "#     complexity = min(len(completed_subtasks)/10.0, 1.0)\n",
    "#     state.append(complexity)\n",
    "\n",
    "#     # Instruction length\n",
    "#     instr_len = min(len(task_instruction)/500.0, 1.0)\n",
    "#     state.append(instr_len)\n",
    "\n",
    "#     # Current subtask length\n",
    "#     sub_len = min(len(current_subtask)/200.0, 1.0)\n",
    "#     state.append(sub_len)\n",
    "\n",
    "#     # Completed subtasks count (normalized)\n",
    "#     state.append(min(len(completed_subtasks)/10.0,1.0))\n",
    "\n",
    "#     # Available agents (all initially available)\n",
    "#     state.extend([1.0, 1.0, 1.0])  # nav, pers, llm\n",
    "\n",
    "#     # Step number normalized\n",
    "#     state.append(min(step_idx/20.0, 1.0))\n",
    "\n",
    "#     # Pad to fixed length\n",
    "#     while len(state) < 16:\n",
    "#         state.append(0.0)\n",
    "\n",
    "#     return state[:16]\n",
    "\n",
    "# def calculate_step_reward(step_idx, total_steps, agent):\n",
    "#     \"\"\"Shaped reward for RL\"\"\"\n",
    "#     reward = 0.1  # small base reward\n",
    "\n",
    "#     if step_idx == 0 and agent == 'llm_agent':\n",
    "#         reward += 0.2\n",
    "#     if step_idx == total_steps - 1 and agent == 'pers_agent':\n",
    "#         reward += 0.2\n",
    "#     if step_idx > 1:\n",
    "#         reward -= 0.05  # penalize repeated steps\n",
    "\n",
    "#     return max(reward, 0.01)\n",
    "\n",
    "# def calculate_returns_to_go(rewards, gamma=0.99):\n",
    "#     \"\"\"Compute returns-to-go\"\"\"\n",
    "#     returns = []\n",
    "#     g = 0.0\n",
    "#     for r in reversed(rewards):\n",
    "#         g = r + gamma * g\n",
    "#         returns.insert(0, g)\n",
    "#     return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49199959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def prepare_rl_transitions(ds):\n",
    "#     transitions = []\n",
    "\n",
    "#     for item in ds:\n",
    "#         task_instruction = item[\"instruct\"]\n",
    "#         subtasks = item[\"sub_tasks\"]\n",
    "#         total_steps = len(subtasks)\n",
    "\n",
    "#         for i, st in enumerate(subtasks):\n",
    "#             current_instruction = st[\"instruct\"]\n",
    "#             agent_name = infer_agent_for_subtask(current_instruction)\n",
    "#             action_id = AGENT_IDS[agent_name]\n",
    "\n",
    "#             numeric_state = create_numeric_state(\n",
    "#                 task_instruction, [s[\"result\"] for s in subtasks[:i]], current_instruction, i, total_steps\n",
    "#             )\n",
    "\n",
    "#             reward = calculate_step_reward(i, total_steps, agent_name)\n",
    "#             done = (i == total_steps - 1)\n",
    "\n",
    "#             transition = {\n",
    "#                 \"state\": numeric_state,\n",
    "#                 \"task_instruction\": task_instruction,\n",
    "#                 \"current_subtask_instruction\": current_instruction,\n",
    "#                 \"completed_subtasks\": [s[\"result\"] for s in subtasks[:i]],\n",
    "#                 \"action\": action_id,\n",
    "#                 \"expected_output\": st[\"result\"],\n",
    "#                 \"reward\": reward,\n",
    "#                 \"done\": done\n",
    "#             }\n",
    "\n",
    "#             transitions.append(transition)\n",
    "\n",
    "#     print(\"Prepared\", len(transitions), \"RL transitions with numeric states and shaped rewards.\")\n",
    "#     return transitions\n",
    "\n",
    "# rl_data = prepare_rl_transitions(ds)\n",
    "# rl_data[:2]  # preview first 2 transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c9ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import random\n",
    "\n",
    "# # ============================================================\n",
    "# # CONFIG\n",
    "# # ============================================================\n",
    "\n",
    "# AGENT_IDS = {\n",
    "#     \"nav_agent\": 0,\n",
    "#     \"pers_agent\": 1,\n",
    "#     \"llm_agent\": 2\n",
    "# }\n",
    "\n",
    "# NUM_AGENTS = len(AGENT_IDS)\n",
    "# STATE_DIM = 16\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # STATE CONSTRUCTION (NO ORACLE SIGNALS)\n",
    "# # ============================================================\n",
    "\n",
    "# def create_numeric_state(\n",
    "#     task_instruction,\n",
    "#     completed_subtasks,\n",
    "#     current_subtask_instruction,\n",
    "#     step_idx,\n",
    "#     total_steps\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create an RL observation vector WITHOUT leaking answers.\n",
    "#     \"\"\"\n",
    "\n",
    "#     state = []\n",
    "\n",
    "#     # Progress through task (allowed)\n",
    "#     state.append(step_idx / max(total_steps, 1))\n",
    "\n",
    "#     # Number of completed subtasks (normalized)\n",
    "#     state.append(len(completed_subtasks) / max(total_steps, 1))\n",
    "\n",
    "#     # Task instruction length (proxy for difficulty)\n",
    "#     state.append(min(len(task_instruction) / 500.0, 1.0))\n",
    "\n",
    "#     # Current subtask length\n",
    "#     state.append(min(len(current_subtask_instruction) / 200.0, 1.0))\n",
    "\n",
    "#     # Total number of subtasks (normalized)\n",
    "#     state.append(min(total_steps / 10.0, 1.0))\n",
    "\n",
    "#     # Agent availability (all available by default)\n",
    "#     state.extend([1.0] * NUM_AGENTS)\n",
    "\n",
    "#     # Remaining steps ratio\n",
    "#     remaining = (total_steps - step_idx - 1) / max(total_steps, 1)\n",
    "#     state.append(max(remaining, 0.0))\n",
    "\n",
    "#     # Padding\n",
    "#     while len(state) < STATE_DIM:\n",
    "#         state.append(0.0)\n",
    "\n",
    "#     return state[:STATE_DIM]\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # DATASET PREPARATION (NO ACTIONS, NO REWARDS)\n",
    "# # ============================================================\n",
    "\n",
    "# def prepare_rl_dataset(raw_dataset):\n",
    "#     \"\"\"\n",
    "#     Prepare dataset for RL environment consumption.\n",
    "\n",
    "#     Output format:\n",
    "#     Each entry represents a decision point.\n",
    "#     \"\"\"\n",
    "\n",
    "#     samples = []\n",
    "\n",
    "#     for item in raw_dataset:\n",
    "#         task_instruction = item[\"instruct\"]\n",
    "#         subtasks = item[\"sub_tasks\"]\n",
    "#         total_steps = len(subtasks)\n",
    "\n",
    "#         completed_results = []\n",
    "\n",
    "#         for step_idx, sub in enumerate(subtasks):\n",
    "\n",
    "#             obs = create_numeric_state(\n",
    "#                 task_instruction=task_instruction,\n",
    "#                 completed_subtasks=completed_results,\n",
    "#                 current_subtask_instruction=sub[\"instruct\"],\n",
    "#                 step_idx=step_idx,\n",
    "#                 total_steps=total_steps\n",
    "#             )\n",
    "\n",
    "#             sample = {\n",
    "#                 # Observation\n",
    "#                 \"state\": obs,\n",
    "\n",
    "#                 # Text context (for env / embedding model)\n",
    "#                 \"task_instruction\": task_instruction,\n",
    "#                 \"current_subtask_instruction\": sub[\"instruct\"],\n",
    "#                 \"completed_subtasks\": completed_results.copy(),\n",
    "\n",
    "#                 # Environment control signals\n",
    "#                 \"step_index\": step_idx,\n",
    "#                 \"total_steps\": total_steps,\n",
    "#                 \"done\": (step_idx == total_steps - 1),\n",
    "\n",
    "#                 # Ground truth execution result (NOT used for reward here)\n",
    "#                 \"expected_output\": sub[\"result\"],\n",
    "\n",
    "#                 # Success flag placeholder (computed by environment)\n",
    "#                 \"success\": None\n",
    "#             }\n",
    "\n",
    "#             samples.append(sample)\n",
    "#             completed_results.append(sub[\"result\"])\n",
    "\n",
    "#     print(f\"Prepared {len(samples)} RL decision points.\")\n",
    "#     return samples\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # OPTIONAL: TRAIN / VAL SPLIT\n",
    "# # ============================================================\n",
    "\n",
    "# def split_dataset(samples, val_ratio=0.1, seed=42):\n",
    "#     random.seed(seed)\n",
    "#     random.shuffle(samples)\n",
    "\n",
    "#     split_idx = int(len(samples) * (1 - val_ratio))\n",
    "#     return samples[:split_idx], samples[split_idx:]\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # USAGE\n",
    "# # ============================================================\n",
    "\n",
    "# # rl_samples = prepare_rl_dataset(ds)\n",
    "# # train_data, val_data = split_dataset(rl_samples)\n",
    "\n",
    "# # ============================================================\n",
    "# # USAGE & SAVE TO FILE\n",
    "# # ============================================================\n",
    "\n",
    "# # If your dataset is already loaded in memory as `ds`, no need to read a file\n",
    "# # ds = ...  # your dataset in memory\n",
    "\n",
    "# # Prepare RL dataset\n",
    "# rl_samples = prepare_rl_dataset(ds)\n",
    "\n",
    "# # Optional: train/validation split\n",
    "# train_data, val_data = split_dataset(rl_samples, val_ratio=0.1)\n",
    "\n",
    "# # Save full RL dataset\n",
    "# OUTPUT_FILE_ALL = \"verigui_coordinator_rlnew.json\"\n",
    "# with open(OUTPUT_FILE_ALL, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(rl_samples, f, indent=2, ensure_ascii=False)\n",
    "# print(f\"✅ Saved full RL dataset to: {OUTPUT_FILE_ALL}\")\n",
    "\n",
    "# # Save train/validation splits\n",
    "# OUTPUT_FILE_TRAIN = \"verigui_coordinator_rl_train.json\"\n",
    "# OUTPUT_FILE_VAL = \"verigui_coordinator_rl_val.json\"\n",
    "\n",
    "# with open(OUTPUT_FILE_TRAIN, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "# with open(OUTPUT_FILE_VAL, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"✅ Saved train split to: {OUTPUT_FILE_TRAIN}\")\n",
    "# print(f\"✅ Saved validation split to: {OUTPUT_FILE_VAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9794c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# OUTPUT_FILE = \"verigui_coordinator_rlnew.json\"\n",
    "\n",
    "# # Save only if we have data\n",
    "# if rl_samples and len(rl_samples) > 0:\n",
    "#     with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(rl_samples, f, indent=2, ensure_ascii=False)\n",
    "#     print(f\"✅ Saved RL-ready dataset to: {OUTPUT_FILE}\")\n",
    "# else:\n",
    "#     print(\"⚠️ No RL samples to save! Check your dataset preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d602076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_FILE = \"verigui_coordinator_rlnew.json\"\n",
    "\n",
    "# with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(rl_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(\"✅ Saved RL-ready dataset to:\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872b229",
   "metadata": {},
   "source": [
    "claude modifications: 15/1/2025  12:47 am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0adcd118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 994 RL transitions.\n",
      "✅ RL dataset fully aligned with coordinator action semantics.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "STATE_DIM = 12\n",
    "ACT_DIM = len(ACTION_SPACE)  # Automatically matches coordinator\n",
    "\n",
    "AGENT_IDS = {\n",
    "    \"nav_agent\": 0,\n",
    "    \"pers_agent\": 1,\n",
    "    \"llm_agent\": 2\n",
    "}\n",
    "NUM_AGENTS = len(AGENT_IDS)\n",
    "\n",
    "# ============================================================\n",
    "# STATE CONSTRUCTION (12-DIM)\n",
    "# ============================================================\n",
    "\n",
    "def create_numeric_state(\n",
    "    task_instruction,\n",
    "    completed_subtasks,\n",
    "    current_subtask_instruction,\n",
    "    step_idx,\n",
    "    total_steps\n",
    "):\n",
    "    state = []\n",
    "\n",
    "    # Temporal progress\n",
    "    state.append(step_idx / max(total_steps, 1))\n",
    "    state.append((total_steps - step_idx - 1) / max(total_steps, 1))\n",
    "\n",
    "    # Completion ratio\n",
    "    state.append(len(completed_subtasks) / max(total_steps, 1))\n",
    "\n",
    "    # Instruction complexity\n",
    "    state.append(min(len(task_instruction) / 500.0, 1.0))\n",
    "    state.append(min(len(current_subtask_instruction) / 200.0, 1.0))\n",
    "\n",
    "    # Agent availability\n",
    "    state.extend([1.0] * NUM_AGENTS)\n",
    "\n",
    "    # Padding\n",
    "    while len(state) < STATE_DIM:\n",
    "        state.append(0.0)\n",
    "\n",
    "    return state[:STATE_DIM]\n",
    "\n",
    "# ============================================================\n",
    "# REWARD FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def compute_reward(prev_progress, new_progress, done, success):\n",
    "    reward = 0.0\n",
    "\n",
    "    # Progress shaping\n",
    "    reward += (new_progress - prev_progress)\n",
    "\n",
    "    # Forward movement bonus\n",
    "    if new_progress > prev_progress:\n",
    "        reward += 0.1\n",
    "\n",
    "    # Terminal reward\n",
    "    if done:\n",
    "        reward += 1.0 if success else -1.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "# ============================================================\n",
    "# DATASET PREPARATION (COORDINATOR-ALIGNED)\n",
    "# ============================================================\n",
    "\n",
    "def prepare_rl_dataset(raw_dataset):\n",
    "    transitions = []\n",
    "    episode_id = 0\n",
    "\n",
    "    for task_id, item in enumerate(raw_dataset):\n",
    "        task_instruction = item[\"instruct\"]\n",
    "        subtasks = item.get(\"sub_tasks\", [])\n",
    "        total_steps = len(subtasks)\n",
    "\n",
    "        completed_results = []\n",
    "        prev_state = None\n",
    "        prev_progress = 0.0\n",
    "        episode_buffer = []\n",
    "\n",
    "        for step_idx, sub in enumerate(subtasks):\n",
    "            state = create_numeric_state(\n",
    "                task_instruction,\n",
    "                completed_results,\n",
    "                sub.get(\"instruct\", \"\"),\n",
    "                step_idx,\n",
    "                total_steps\n",
    "            )\n",
    "\n",
    "            # -----------------------------\n",
    "            # Semantic action mapping\n",
    "            # -----------------------------\n",
    "            action = infer_action_from_subtask(sub.get(\"instruct\", \"\"))\n",
    "\n",
    "            progress = step_idx / max(total_steps, 1)\n",
    "            done = (step_idx == total_steps - 1)\n",
    "            success = done  # Offline assumption: last step succeeds\n",
    "\n",
    "            reward = compute_reward(prev_progress, progress, done, success)\n",
    "\n",
    "            if prev_state is not None:\n",
    "                episode_buffer.append({\n",
    "                    \"task_id\": task_id,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"step_id\": step_idx,\n",
    "                    \"state\": prev_state,\n",
    "                    \"action\": action,\n",
    "                    \"reward\": reward,\n",
    "                    \"next_state\": state,\n",
    "                    \"done\": done,\n",
    "                    \"return_to_go\": None,\n",
    "                    \"action_name\": ACTION_SPACE[action],\n",
    "                    \"termination_type\": \"success\" if done and success else None\n",
    "                })\n",
    "\n",
    "            prev_state = state\n",
    "            prev_progress = progress\n",
    "            completed_results.append(sub.get(\"result\", \"\"))\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # RETURN-TO-GO COMPUTATION\n",
    "        # ----------------------------------------------------\n",
    "        G = 0.0\n",
    "        for t in reversed(episode_buffer):\n",
    "            G = t[\"reward\"] + GAMMA * G\n",
    "            t[\"return_to_go\"] = G\n",
    "\n",
    "        transitions.extend(episode_buffer)\n",
    "        episode_id += 1\n",
    "\n",
    "    print(f\"Prepared {len(transitions)} RL transitions.\")\n",
    "    return transitions\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / VAL SPLIT\n",
    "# ============================================================\n",
    "\n",
    "def split_dataset(samples, val_ratio=0.1, seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    random.shuffle(samples)\n",
    "    split_idx = int(len(samples) * (1 - val_ratio))\n",
    "    return samples[:split_idx], samples[split_idx:]\n",
    "\n",
    "# ============================================================\n",
    "# USAGE\n",
    "# ============================================================\n",
    "\n",
    "# ds must be loaded beforehand:\n",
    "# with open(\"raw_dataset.json\") as f:\n",
    "#     ds = json.load(f)\n",
    "\n",
    "rl_transitions = prepare_rl_dataset(ds)\n",
    "train_data, val_data = split_dataset(rl_transitions)\n",
    "\n",
    "with open(\"verigui_coordinator_rl_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"verigui_coordinator_rl_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ RL dataset fully aligned with coordinator action semantics.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
