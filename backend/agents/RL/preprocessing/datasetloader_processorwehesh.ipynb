{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643f45ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VeriGUI with 302 tasks (split='test').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'folder': '675',\n",
       " 'instruct': 'Find the Italian film that won the Grand Prix du Jury at the 1989 Cannes Film Festival, and was written and directed by Giuseppe Tornatore. For this film, please provide the following information: the name of the actor who played the blind projectionist Alfredo and won the BAFTA Award for Best Actor for this role; a famous quote about \"life\" spoken by this actor in the film; the specific award jointly received by the film\\'s composer and his son; the duration of the film\\'s longest version; the film\\'s initial release date in Italy; the name of the designer who won the César Award for Best Poster; and the film\\'s box office gross in the United States and Canada.',\n",
       " 'result': [\"film's title:Cinema Paradiso\",\n",
       "  \"the actor's name:Philippe Noiret\",\n",
       "  'line spoken:Life isn’t like in the movies. Life… is much harder',\n",
       "  'the names of the composer and his son:Ennio Morricone,Andrea Morricone,specific award:BAFTA Award for For Best Original Film Music',\n",
       "  'duration:174min',\n",
       "  \" the film's original release date in Italy:29 September 1988\",\n",
       "  'the name of the designer:Jouineau Bourduge',\n",
       "  'box office data:$12.3 million'],\n",
       " 'sub_tasks': [{'instruct': \"Collect a list of films that won the Grand Prix du Jury at the 1989 Cannes Film Festival, recording each film's title, director, screenwriter, and country of production.\",\n",
       "   'result': ['Cinema Paradiso,Giuseppe Tornatore,Giuseppe Tornatore,Italy',\n",
       "    'Too Beautiful for You,Bertrand Blier,Bertrand Blier,France']},\n",
       "  {'instruct': 'From the above list, filter out films written and directed by Giuseppe Tornatore, and whose country of production is Italy, to identify the target film.',\n",
       "   'result': ['Cinema Paradiso']},\n",
       "  {'instruct': 'Look up the cast and crew of the target film, identify the name of the actor who played \"Alfredo\", and consult the BAFTA Award records for \"Best Actor\" in the corresponding year to obtain the actor\\'s name.',\n",
       "   'result': ['Philippe Noiret']},\n",
       "  {'instruct': 'Search the target film\\'s script, subtitle files, or authoritative movie quote databases to extract a line spoken by the character \"Alfredo\" that contains a theme about \"life\".',\n",
       "   'result': ['Life isn’t like in the movies. Life… is much harder']},\n",
       "  {'instruct': \"Obtain the names of the target film's composer and his son, and search major film award databases (such as the David di Donatello Awards, BAFTA Awards, etc.) to find the specific award they jointly received for the film.\",\n",
       "   'result': ['Ennio Morricone,Andrea Morricone',\n",
       "    'BAFTA Award for For Best Original Film Music']},\n",
       "  {'instruct': 'Query movie databases (such as IMDb, TCM, etc.) for duration records of all versions of the film, and extract the longest duration data (in minutes).',\n",
       "   'result': ['174min']},\n",
       "  {'instruct': 'Query the film\\'s original release date in Italy (year, month, day), the name of the designer who won the César Award for \"Best Poster\", and the total opening weekend box office data in the United States and Canada.',\n",
       "   'result': ['29 September 1988', 'Jouineau Bourduge', '$12.3 million']}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_verigui():\n",
    "    ds_dict = load_dataset(\"2077AIDataFoundation/VeriGUI\")\n",
    "    # Use the only available split: \"test\"\n",
    "    ds = ds_dict[\"test\"]\n",
    "    print(f\"Loaded VeriGUI with {len(ds)} tasks (split='test').\")\n",
    "    return ds\n",
    "\n",
    "ds = load_verigui()\n",
    "ds[0]  # preview first example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b65a85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map agent names to integer IDs\n",
    "# AGENT_IDS = {\"nav_agent\": 0, \"pers_agent\": 1, \"llm_agent\": 2}\n",
    "\n",
    "# def infer_agent_for_subtask(sub_instruction):\n",
    "#     \"\"\"Heuristic agent assignment based on subtask instruction\"\"\"\n",
    "#     inst = sub_instruction.lower()\n",
    "#     if any(k in inst for k in ['search', 'find', 'look up', 'navigate', 'click', 'scroll']):\n",
    "#         return 'nav_agent'\n",
    "#     if any(k in inst for k in ['filter', 'analyze', 'compare', 'extract', 'identify', 'plan']):\n",
    "#         return 'llm_agent'\n",
    "#     if any(k in inst for k in ['suggest', 'recommend', 'adapt', 'personalize']):\n",
    "#         return 'pers_agent'\n",
    "#     # default alternating\n",
    "#     return 'llm_agent'\n",
    "\n",
    "# def create_numeric_state(task_instruction, completed_subtasks, current_subtask, step_idx, total_steps):\n",
    "#     \"\"\"Create a numeric state vector\"\"\"\n",
    "#     state = []\n",
    "\n",
    "#     # Progress\n",
    "#     progress = step_idx / total_steps if total_steps > 0 else 0.0\n",
    "#     state.append(progress)\n",
    "\n",
    "#     # Task complexity\n",
    "#     complexity = min(len(completed_subtasks)/10.0, 1.0)\n",
    "#     state.append(complexity)\n",
    "\n",
    "#     # Instruction length\n",
    "#     instr_len = min(len(task_instruction)/500.0, 1.0)\n",
    "#     state.append(instr_len)\n",
    "\n",
    "#     # Current subtask length\n",
    "#     sub_len = min(len(current_subtask)/200.0, 1.0)\n",
    "#     state.append(sub_len)\n",
    "\n",
    "#     # Completed subtasks count (normalized)\n",
    "#     state.append(min(len(completed_subtasks)/10.0,1.0))\n",
    "\n",
    "#     # Available agents (all initially available)\n",
    "#     state.extend([1.0, 1.0, 1.0])  # nav, pers, llm\n",
    "\n",
    "#     # Step number normalized\n",
    "#     state.append(min(step_idx/20.0, 1.0))\n",
    "\n",
    "#     # Pad to fixed length\n",
    "#     while len(state) < 16:\n",
    "#         state.append(0.0)\n",
    "\n",
    "#     return state[:16]\n",
    "\n",
    "# def calculate_step_reward(step_idx, total_steps, agent):\n",
    "#     \"\"\"Shaped reward for RL\"\"\"\n",
    "#     reward = 0.1  # small base reward\n",
    "\n",
    "#     if step_idx == 0 and agent == 'llm_agent':\n",
    "#         reward += 0.2\n",
    "#     if step_idx == total_steps - 1 and agent == 'pers_agent':\n",
    "#         reward += 0.2\n",
    "#     if step_idx > 1:\n",
    "#         reward -= 0.05  # penalize repeated steps\n",
    "\n",
    "#     return max(reward, 0.01)\n",
    "\n",
    "# def calculate_returns_to_go(rewards, gamma=0.99):\n",
    "#     \"\"\"Compute returns-to-go\"\"\"\n",
    "#     returns = []\n",
    "#     g = 0.0\n",
    "#     for r in reversed(rewards):\n",
    "#         g = r + gamma * g\n",
    "#         returns.insert(0, g)\n",
    "#     return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49199959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def prepare_rl_transitions(ds):\n",
    "#     transitions = []\n",
    "\n",
    "#     for item in ds:\n",
    "#         task_instruction = item[\"instruct\"]\n",
    "#         subtasks = item[\"sub_tasks\"]\n",
    "#         total_steps = len(subtasks)\n",
    "\n",
    "#         for i, st in enumerate(subtasks):\n",
    "#             current_instruction = st[\"instruct\"]\n",
    "#             agent_name = infer_agent_for_subtask(current_instruction)\n",
    "#             action_id = AGENT_IDS[agent_name]\n",
    "\n",
    "#             numeric_state = create_numeric_state(\n",
    "#                 task_instruction, [s[\"result\"] for s in subtasks[:i]], current_instruction, i, total_steps\n",
    "#             )\n",
    "\n",
    "#             reward = calculate_step_reward(i, total_steps, agent_name)\n",
    "#             done = (i == total_steps - 1)\n",
    "\n",
    "#             transition = {\n",
    "#                 \"state\": numeric_state,\n",
    "#                 \"task_instruction\": task_instruction,\n",
    "#                 \"current_subtask_instruction\": current_instruction,\n",
    "#                 \"completed_subtasks\": [s[\"result\"] for s in subtasks[:i]],\n",
    "#                 \"action\": action_id,\n",
    "#                 \"expected_output\": st[\"result\"],\n",
    "#                 \"reward\": reward,\n",
    "#                 \"done\": done\n",
    "#             }\n",
    "\n",
    "#             transitions.append(transition)\n",
    "\n",
    "#     print(\"Prepared\", len(transitions), \"RL transitions with numeric states and shaped rewards.\")\n",
    "#     return transitions\n",
    "\n",
    "# rl_data = prepare_rl_transitions(ds)\n",
    "# rl_data[:2]  # preview first 2 transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1300 RL decision points.\n",
      "✅ Saved full RL dataset to: verigui_coordinator_rlnew.json\n",
      "✅ Saved train split to: verigui_coordinator_rl_train.json\n",
      "✅ Saved validation split to: verigui_coordinator_rl_val.json\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import random\n",
    "\n",
    "# # ============================================================\n",
    "# # CONFIG\n",
    "# # ============================================================\n",
    "\n",
    "# AGENT_IDS = {\n",
    "#     \"nav_agent\": 0,\n",
    "#     \"pers_agent\": 1,\n",
    "#     \"llm_agent\": 2\n",
    "# }\n",
    "\n",
    "# NUM_AGENTS = len(AGENT_IDS)\n",
    "# STATE_DIM = 16\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # STATE CONSTRUCTION (NO ORACLE SIGNALS)\n",
    "# # ============================================================\n",
    "\n",
    "# def create_numeric_state(\n",
    "#     task_instruction,\n",
    "#     completed_subtasks,\n",
    "#     current_subtask_instruction,\n",
    "#     step_idx,\n",
    "#     total_steps\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create an RL observation vector WITHOUT leaking answers.\n",
    "#     \"\"\"\n",
    "\n",
    "#     state = []\n",
    "\n",
    "#     # Progress through task (allowed)\n",
    "#     state.append(step_idx / max(total_steps, 1))\n",
    "\n",
    "#     # Number of completed subtasks (normalized)\n",
    "#     state.append(len(completed_subtasks) / max(total_steps, 1))\n",
    "\n",
    "#     # Task instruction length (proxy for difficulty)\n",
    "#     state.append(min(len(task_instruction) / 500.0, 1.0))\n",
    "\n",
    "#     # Current subtask length\n",
    "#     state.append(min(len(current_subtask_instruction) / 200.0, 1.0))\n",
    "\n",
    "#     # Total number of subtasks (normalized)\n",
    "#     state.append(min(total_steps / 10.0, 1.0))\n",
    "\n",
    "#     # Agent availability (all available by default)\n",
    "#     state.extend([1.0] * NUM_AGENTS)\n",
    "\n",
    "#     # Remaining steps ratio\n",
    "#     remaining = (total_steps - step_idx - 1) / max(total_steps, 1)\n",
    "#     state.append(max(remaining, 0.0))\n",
    "\n",
    "#     # Padding\n",
    "#     while len(state) < STATE_DIM:\n",
    "#         state.append(0.0)\n",
    "\n",
    "#     return state[:STATE_DIM]\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # DATASET PREPARATION (NO ACTIONS, NO REWARDS)\n",
    "# # ============================================================\n",
    "\n",
    "# def prepare_rl_dataset(raw_dataset):\n",
    "#     \"\"\"\n",
    "#     Prepare dataset for RL environment consumption.\n",
    "\n",
    "#     Output format:\n",
    "#     Each entry represents a decision point.\n",
    "#     \"\"\"\n",
    "\n",
    "#     samples = []\n",
    "\n",
    "#     for item in raw_dataset:\n",
    "#         task_instruction = item[\"instruct\"]\n",
    "#         subtasks = item[\"sub_tasks\"]\n",
    "#         total_steps = len(subtasks)\n",
    "\n",
    "#         completed_results = []\n",
    "\n",
    "#         for step_idx, sub in enumerate(subtasks):\n",
    "\n",
    "#             obs = create_numeric_state(\n",
    "#                 task_instruction=task_instruction,\n",
    "#                 completed_subtasks=completed_results,\n",
    "#                 current_subtask_instruction=sub[\"instruct\"],\n",
    "#                 step_idx=step_idx,\n",
    "#                 total_steps=total_steps\n",
    "#             )\n",
    "\n",
    "#             sample = {\n",
    "#                 # Observation\n",
    "#                 \"state\": obs,\n",
    "\n",
    "#                 # Text context (for env / embedding model)\n",
    "#                 \"task_instruction\": task_instruction,\n",
    "#                 \"current_subtask_instruction\": sub[\"instruct\"],\n",
    "#                 \"completed_subtasks\": completed_results.copy(),\n",
    "\n",
    "#                 # Environment control signals\n",
    "#                 \"step_index\": step_idx,\n",
    "#                 \"total_steps\": total_steps,\n",
    "#                 \"done\": (step_idx == total_steps - 1),\n",
    "\n",
    "#                 # Ground truth execution result (NOT used for reward here)\n",
    "#                 \"expected_output\": sub[\"result\"],\n",
    "\n",
    "#                 # Success flag placeholder (computed by environment)\n",
    "#                 \"success\": None\n",
    "#             }\n",
    "\n",
    "#             samples.append(sample)\n",
    "#             completed_results.append(sub[\"result\"])\n",
    "\n",
    "#     print(f\"Prepared {len(samples)} RL decision points.\")\n",
    "#     return samples\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # OPTIONAL: TRAIN / VAL SPLIT\n",
    "# # ============================================================\n",
    "\n",
    "# def split_dataset(samples, val_ratio=0.1, seed=42):\n",
    "#     random.seed(seed)\n",
    "#     random.shuffle(samples)\n",
    "\n",
    "#     split_idx = int(len(samples) * (1 - val_ratio))\n",
    "#     return samples[:split_idx], samples[split_idx:]\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # USAGE\n",
    "# # ============================================================\n",
    "\n",
    "# # rl_samples = prepare_rl_dataset(ds)\n",
    "# # train_data, val_data = split_dataset(rl_samples)\n",
    "\n",
    "# # ============================================================\n",
    "# # USAGE & SAVE TO FILE\n",
    "# # ============================================================\n",
    "\n",
    "# # If your dataset is already loaded in memory as `ds`, no need to read a file\n",
    "# # ds = ...  # your dataset in memory\n",
    "\n",
    "# # Prepare RL dataset\n",
    "# rl_samples = prepare_rl_dataset(ds)\n",
    "\n",
    "# # Optional: train/validation split\n",
    "# train_data, val_data = split_dataset(rl_samples, val_ratio=0.1)\n",
    "\n",
    "# # Save full RL dataset\n",
    "# OUTPUT_FILE_ALL = \"verigui_coordinator_rlnew.json\"\n",
    "# with open(OUTPUT_FILE_ALL, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(rl_samples, f, indent=2, ensure_ascii=False)\n",
    "# print(f\"✅ Saved full RL dataset to: {OUTPUT_FILE_ALL}\")\n",
    "\n",
    "# # Save train/validation splits\n",
    "# OUTPUT_FILE_TRAIN = \"verigui_coordinator_rl_train.json\"\n",
    "# OUTPUT_FILE_VAL = \"verigui_coordinator_rl_val.json\"\n",
    "\n",
    "# with open(OUTPUT_FILE_TRAIN, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "# with open(OUTPUT_FILE_VAL, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"✅ Saved train split to: {OUTPUT_FILE_TRAIN}\")\n",
    "# print(f\"✅ Saved validation split to: {OUTPUT_FILE_VAL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9794c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved RL-ready dataset to: verigui_coordinator_rlnew.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "OUTPUT_FILE = \"verigui_coordinator_rlnew.json\"\n",
    "\n",
    "# Save only if we have data\n",
    "if rl_samples and len(rl_samples) > 0:\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rl_samples, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✅ Saved RL-ready dataset to: {OUTPUT_FILE}\")\n",
    "else:\n",
    "    print(\"⚠️ No RL samples to save! Check your dataset preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d602076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_FILE = \"verigui_coordinator_rlnew.json\"\n",
    "\n",
    "# with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(rl_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(\"✅ Saved RL-ready dataset to:\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872b229",
   "metadata": {},
   "source": [
    "claude modifications: 15/1/2025  12:47 am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0adcd118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 998 RL transitions.\n",
      "✅ RL-ready dataset with task_id saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "MAX_ACTION_ID = 12\n",
    "STATE_DIM = 12\n",
    "\n",
    "AGENT_IDS = {\n",
    "    \"nav_agent\": 0,\n",
    "    \"pers_agent\": 1,\n",
    "    \"llm_agent\": 2\n",
    "}\n",
    "NUM_AGENTS = len(AGENT_IDS)\n",
    "\n",
    "# ============================================================\n",
    "# ACTION SPACE\n",
    "# ============================================================\n",
    "\n",
    "ACTION_SPACE = {\n",
    "    0: \"search_web\",\n",
    "    1: \"search_database\",\n",
    "    2: \"compute_statistic\",\n",
    "    3: \"filter_data\",\n",
    "    4: \"aggregate_results\",\n",
    "    5: \"compare_values\",\n",
    "    6: \"identify_maximum\",\n",
    "    7: \"identify_minimum\",\n",
    "    8: \"extract_entity\",\n",
    "    9: \"verify_source\",\n",
    "    10: \"summarize\",\n",
    "    11: \"reason\",\n",
    "    12: \"final_answer\"\n",
    "}\n",
    "\n",
    "def sample_action(exploration=False):\n",
    "    if exploration:\n",
    "        return random.randint(0, MAX_ACTION_ID)\n",
    "    return random.choice(list(ACTION_SPACE.keys()))\n",
    "\n",
    "# ============================================================\n",
    "# STATE CONSTRUCTION (PRUNED + TEMPORAL)\n",
    "# ============================================================\n",
    "\n",
    "def create_numeric_state(\n",
    "    task_instruction,\n",
    "    completed_subtasks,\n",
    "    current_subtask_instruction,\n",
    "    step_idx,\n",
    "    total_steps\n",
    "):\n",
    "    state = []\n",
    "\n",
    "    # Temporal progress\n",
    "    state.append(step_idx / max(total_steps, 1))\n",
    "    state.append((total_steps - step_idx - 1) / max(total_steps, 1))\n",
    "\n",
    "    # Completion ratio\n",
    "    state.append(len(completed_subtasks) / max(total_steps, 1))\n",
    "\n",
    "    # Difficulty proxies\n",
    "    state.append(min(len(task_instruction) / 500.0, 1.0))\n",
    "    state.append(min(len(current_subtask_instruction) / 200.0, 1.0))\n",
    "\n",
    "    # Agent availability\n",
    "    state.extend([1.0] * NUM_AGENTS)\n",
    "\n",
    "    # Padding\n",
    "    while len(state) < STATE_DIM:\n",
    "        state.append(0.0)\n",
    "\n",
    "    return state[:STATE_DIM]\n",
    "\n",
    "# ============================================================\n",
    "# REWARD FUNCTION (DENSE + TERMINAL)\n",
    "# ============================================================\n",
    "\n",
    "def compute_reward(prev_progress, new_progress, done, success):\n",
    "    reward = 0.0\n",
    "\n",
    "    # Progress shaping\n",
    "    reward += (new_progress - prev_progress)\n",
    "\n",
    "    # Subtask completion bonus\n",
    "    if new_progress > prev_progress:\n",
    "        reward += 0.1\n",
    "\n",
    "    # Terminal reward\n",
    "    if done:\n",
    "        reward += 1.0 if success else -1.0\n",
    "\n",
    "    return reward\n",
    "\n",
    "# ============================================================\n",
    "# DATASET PREPARATION (FULL RL TRANSITIONS)\n",
    "# ============================================================\n",
    "\n",
    "def prepare_rl_dataset(raw_dataset, exploration_ratio=0.3):\n",
    "    transitions = []\n",
    "    episode_id = 0\n",
    "\n",
    "    for task_id, item in enumerate(raw_dataset):\n",
    "        task_instruction = item[\"instruct\"]\n",
    "        subtasks = item[\"sub_tasks\"]\n",
    "        total_steps = len(subtasks)\n",
    "\n",
    "        completed_results = []\n",
    "        prev_state = None\n",
    "        prev_progress = 0.0\n",
    "        episode_buffer = []\n",
    "\n",
    "        for step_idx, sub in enumerate(subtasks):\n",
    "            state = create_numeric_state(\n",
    "                task_instruction,\n",
    "                completed_results,\n",
    "                sub[\"instruct\"],\n",
    "                step_idx,\n",
    "                total_steps\n",
    "            )\n",
    "\n",
    "            action = sample_action(\n",
    "                exploration=(random.random() < exploration_ratio)\n",
    "            )\n",
    "\n",
    "            progress = step_idx / max(total_steps, 1)\n",
    "            done = (step_idx == total_steps - 1)\n",
    "\n",
    "            # Offline assumption: last step = success\n",
    "            success = done\n",
    "\n",
    "            reward = compute_reward(\n",
    "                prev_progress,\n",
    "                progress,\n",
    "                done,\n",
    "                success\n",
    "            )\n",
    "\n",
    "            if prev_state is not None:\n",
    "                episode_buffer.append({\n",
    "                    # -------- IDs --------\n",
    "                    \"task_id\": task_id,\n",
    "                    \"episode_id\": episode_id,\n",
    "                    \"step_id\": step_idx,\n",
    "\n",
    "                    # -------- RL core --------\n",
    "                    \"state\": prev_state,\n",
    "                    \"action\": action,\n",
    "                    \"reward\": reward,\n",
    "                    \"next_state\": state,\n",
    "                    \"done\": done,\n",
    "\n",
    "                    # -------- Metadata --------\n",
    "                    \"termination_type\": (\n",
    "                        \"success\" if done and success else\n",
    "                        \"failure\" if done else\n",
    "                        None\n",
    "                    )\n",
    "                })\n",
    "\n",
    "            prev_state = state\n",
    "            prev_progress = progress\n",
    "            completed_results.append(sub[\"result\"])\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # RETURN-TO-GO COMPUTATION\n",
    "        # ----------------------------------------------------\n",
    "        G = 0.0\n",
    "        for t in reversed(episode_buffer):\n",
    "            G = t[\"reward\"] + GAMMA * G\n",
    "            t[\"return_to_go\"] = G\n",
    "\n",
    "        transitions.extend(episode_buffer)\n",
    "        episode_id += 1\n",
    "\n",
    "    print(f\"Prepared {len(transitions)} RL transitions.\")\n",
    "    return transitions\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / VAL SPLIT\n",
    "# ============================================================\n",
    "\n",
    "def split_dataset(samples, val_ratio=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(samples)\n",
    "    split_idx = int(len(samples) * (1 - val_ratio))\n",
    "    return samples[:split_idx], samples[split_idx:]\n",
    "\n",
    "# ============================================================\n",
    "# USAGE\n",
    "# ============================================================\n",
    "\n",
    "# ds must already be loaded in the notebook\n",
    "# ds = [...]\n",
    "\n",
    "rl_transitions = prepare_rl_dataset(ds)\n",
    "\n",
    "train_data, val_data = split_dataset(rl_transitions)\n",
    "\n",
    "with open(\"verigui_coordinator_rl_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"verigui_coordinator_rl_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ RL-ready dataset with task_id saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
